---
layout:     post
title:      对率回归
subtitle:   Logistic Regression
date:       2021-09-20
author:     Welt Xing
header-img: img/home-bg.jpg
catalog:    true
tags:
    - 机器学习
    - 分类
---

对于很多机器学习入门者，对率回归（Logistic Regression）是第一个学到的**分类模型**。我们这里将实现一个逻辑回归模型。

## 问题描述

我们在前面提到过[感知机算法](https://welts.xyz/2021/04/25/perceptron/)，该算法试图寻找一组参数$w_i,b$，形成的超平面：

$$
\sum_{i=1}^nw_ix_i+b=0
$$

将一组二分类训练数据划分成两部分。但感知机算法在超平面将两类数据划分就停止，不会考虑其泛化性。比如对于下面的样本，有三种感知机训练出的可行划分平面：

![3model](/img/3perceptron.png)

但在这三种模型中，我们更原因选择位于两类样本“中间”的划分超平面model2 ，因为它对训练样本的扰动“容忍”性最好，换句话说就是“泛化能力更强”。如果从“距离”上寻找，最优的超平面就是支持向量机模型；如果从“概率”上研究，最优的超平面就是对率回归对应的超平面：在该超平面上所有的样本，划分为正负类的概率都是50\%；离超平面距离越大，分为某一类的概率就越大。

> 笔者认为这样看待感知机，SVM和对率回归，也可以解释为什么SVM分类中没有概率语义。

## 问题形式化

在对率回归中，我们选用Sigmoid函数来进行距离到概率的转换：

$$
\begin{aligned}
f(z)&=\dfrac{1}{1+\exp(-z)}\\
\Pr(y=1\vert x)&=\dfrac{1}{1+\exp\big(-(wx+b)\big)}\\
&=\dfrac{\exp(wx+b)}{1+\exp(wx+b)}\\
\Pr(y=0\vert x)&=1-\Pr(c=1\vert x)\\
&=\dfrac{1}{1+\exp(wx+b)}
\end{aligned}
$$

上式可以合并为一个式子：

$$
\Pr(y\vert x)=\Pr(y=0\vert x)^{1-y}\Pr(y=1\vert x)^{y}
$$

Sigmoid图像是这样的：

<img src="https://welts.xyz/img/act1.png" alt="sigmoid" style="zoom: 67%;" />

可以看到，从$-\infty$到$\infty$的过程中，函数单调增，而且在自变量极小或极大的时候概率趋向于收敛到0和1，在决策边界（$z=0$，也就是$wx+b=0$）附近时概率变化大，分类不清晰。

我们的目标是找出超平面，能够使各样本被归为自身类别的概率尽可能大。由此写出目标函数：

$$
\begin{aligned}
L&=\prod_{i=1}^n\Pr(y_i\vert x_i)
\end{aligned}
$$

目标是极大化$L$，为了便于计算，等价于极小化

$$
\begin{aligned}
-\frac1{n}\ln L&=-\frac1n\sum_{i=1}^n\ln\Pr(y_i\vert x_i)\\
&=-\frac1n\sum_{i=1}^n\bigg(y_i\ln\Pr(y_i=1\vert x_i)+(1-y_i)\ln\Pr(y_i=0\vert x_i)\bigg)\\
&=-\frac1n\sum_{i=1}^n\bigg(y_i\ln\dfrac{\exp(wx+b)}{1+\exp(wx+b)}+(1-y_i)\ln\dfrac{1}{1+\exp(wx+b)} \bigg)\\
f&=-\frac1n\sum_{i=1}^n\bigg(y_i\theta^\top \hat{x}_i-\ln(1+\exp(\theta^\top \hat{x}_i)) \bigg)
\end{aligned}
$$

这里我们将$wx+b$简写成等价形式$\theta^\top\hat{x}$，因为

$$
wx+b=\begin{bmatrix}
w&b
\end{bmatrix}\begin{bmatrix}
x\\1
\end{bmatrix}=\theta^\top \hat{x}
$$

## 问题求解

到此，我们就可以去通过梯度下降，牛顿法等优化算法去优化该问题。

$$
\begin{aligned}
\dfrac{\partial f}{\partial\theta}&=-\frac1n\sum_{i=1}^n\bigg(y_i\hat{x}_i-\dfrac{\exp(\theta^\top\hat{x}_i)}{1+\exp(\theta^\top\hat{x}_i)}\hat x_i
\bigg)\\
&=\frac1n\sum_{i=1}^n\bigg(\dfrac{\exp(\theta^\top\hat{x}_i)}{1+\exp(\theta^\top\hat{x}_i)}-y_i\bigg)\hat x_i\\
&=\frac1n\sum_{i=1}^n\bigg(\text{Sigmoid}(\theta^\top\hat x_i)-y_i\bigg)\hat{x}_i
\end{aligned}
$$

用Numpy实现：

```python
def Sigmoid(x):
    return 1 / (1 + np.exp(-x))

for i in range(max_iter):
    theta -= lr * np.mean((Sigmoid(X @ theta) - y) * X.T, axis=1)
```
