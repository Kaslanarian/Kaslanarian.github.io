---
layout:     post
title:      启发式搜索与演化算法
date:       2022-01-04
author:     Welt Xing
header-img: img/hsae/hsae.png
catalog:    true
tags:
    - 演化算法
    - 课程
---

- [经典搜索与启发式搜索](#经典搜索与启发式搜索)
- [搜索与优化，一些优化方法](#搜索与优化一些优化方法)
  - [爬山法](#爬山法)
  - [爬山法变种](#爬山法变种)
  - [模拟退火](#模拟退火)
  - [局部束搜索](#局部束搜索)
  - [连续空间下的搜索](#连续空间下的搜索)
  - [演化算法](#演化算法)
- [演化算法(EA)](#演化算法ea)
  - [算法框架](#算法框架)
    - [解的表示(representation)](#解的表示representation)
    - [合适度(fitness)](#合适度fitness)
    - [种群(population)](#种群population)
    - [初始化(initialization)](#初始化initialization)
    - [父代选择(parent selection)](#父代选择parent-selection)
    - [变异(variation)](#变异variation)
    - [幸存筛选(survivor selection)](#幸存筛选survivor-selection)
    - [停止准则(stop criterion)](#停止准则stop-criterion)
    - [例子](#例子)
  - [解表示、突变和重组](#解表示突变和重组)
    - [二进制表示(Binary representation)](#二进制表示binary-representation)
      - [二进制表示下的突变](#二进制表示下的突变)
      - [二进制表示下的重组](#二进制表示下的重组)
    - [整数表示](#整数表示)
      - [整数表示下的突变](#整数表示下的突变)
      - [整数表示下的重组](#整数表示下的重组)
    - [实数表示](#实数表示)
      - [实数表示下的突变](#实数表示下的突变)
    - [实数表示下的重组](#实数表示下的重组)
    - [排列表示](#排列表示)
      - [排列表示下的变异](#排列表示下的变异)
        - [交换变异](#交换变异)
        - [插入变异](#插入变异)
        - [争夺变异](#争夺变异)
        - [逆转变异](#逆转变异)
      - [排列表示下的重组](#排列表示下的重组)
        - [Partially mapped crossover](#partially-mapped-crossover)
        - [Edge crossover](#edge-crossover)
        - [Order crossover](#order-crossover)
        - [Cycle crossover](#cycle-crossover)
    - [树表示](#树表示)
      - [树表示的突变](#树表示的突变)
      - [树表示的重组](#树表示的重组)
    - [关于突变与重组](#关于突变与重组)
  - [筛选与多样性](#筛选与多样性)
    - [父辈选择](#父辈选择)
      - [Fitness proportional selection](#fitness-proportional-selection)
      - [Ranking selection](#ranking-selection)
    - [采样](#采样)
      - [Roulette wheel](#roulette-wheel)
      - [Stochastic universal sampling](#stochastic-universal-sampling)
      - [Uniform selection](#uniform-selection)
    - [幸存筛选](#幸存筛选)
      - [Age-based replacement](#age-based-replacement)
      - [Fitness-based replacement](#fitness-based-replacement)
    - [种群多样性](#种群多样性)
      - [Fitness sharing](#fitness-sharing)
      - [Crowding](#crowding)
  - [演化算法的流行变种](#演化算法的流行变种)
    - [遗传算法](#遗传算法)
    - [进化策略算法](#进化策略算法)
    - [进化编程](#进化编程)
    - [遗传编程](#遗传编程)
    - [差分演化](#差分演化)
    - [粒子群算法](#粒子群算法)
    - [蚁群算法](#蚁群算法)
    - [分布估计算法](#分布估计算法)
- [演化算法的理论分析](#演化算法的理论分析)
- [演化算法与优化](#演化算法与优化)
  - [多目标优化](#多目标优化)
  - [带约束优化](#带约束优化)

## 经典搜索与启发式搜索

一个经典的搜索问题由下面五个要素构成：

- 初始状态；
- 动作空间；
- 转移模型；
- 目标状态；
- 路径开销.

搜索问题的**解**是一个路径，也就是一个从初始状态到目标状态的动作序列；而搜索问题的**最优解**，是寻找一个代价最低的路径。

## 搜索与优化，一些优化方法

优化问题的最优解：

$$
\arg\min_x h(x)
$$

或

$$
\arg\max_x f(x)
$$

也是寻找一个使函数值最小(大)的点。如果将搜索问题的每一种解视作一个点，那么搜索问题就可以被视作一个优化问题。我们接下来介绍几种优化问题的求解方法。

### 爬山法

爬山法是一种简单的循环过程，不断向值增加的方向移动，算法在到达一个“峰顶”的时候停止：

![image-20211109154857732](/img/hsae/image-20211109154857732.png)

我们分别从搜索问题和优化问题来讨论该算法。以八皇后问题为例，它是一个经典的搜索问题，我们用**当前互相攻击的Queen对数**作为启发式函数的值。比如下图的状态

<img src="/img/hsae/image-20211109155232142.png" alt="image-20211109155232142" style="zoom:67%;" />

其启发式函数的值为17，表示有17对横向、纵向或对角向互相冲突的棋子。毫无疑问，我们想让启发式函数的值越小越好，就是要去启发式函数值最小的**邻居状态**，这里的邻居状态就是移动一个棋子一次后的棋盘。

对于优化问题，爬山法显得更为直观，以一元函数的优化为例：
$$
\arg\max_x f(x)
$$
当前状态就是坐标轴上一个点$x$。

<img src="/img/hsae/hill_climb1.png" alt="hill_climb" style="zoom:67%;" />

它的邻居状态就是它的左边$x-\varepsilon$和右边$x+\varepsilon(\varepsilon>0)$。根据算法，它应该转移到左边的邻居状态。

爬山法停止有以下几种可能：

1. 到达全局最优(global maximum)；
2. 到达局部最优(local maximum)；
3. 到达平坦的局部最优（flat local maximum）；
4. 到达平坦的非最优处(shoulder).

就像下图所示：

![image-20211109160658528](/img/hsae/image-20211109160658528.png)

显然，爬山法存在缺陷，需要改进。

### 爬山法变种

- 随机重启爬山法（Random-restart hill-climbing search）基于这样的思想：**如果一开始不成功，那么尝试再尝试**，也就是随机生成初始状态来引导爬山法搜索。该算法完备的概率趋近于1，因为它最终会生成一个目标状态作为初始状态；
- 随机爬山法（Stochastic hill-climbing）会寻找所有更优的邻居状态，然后随机选择一个作为下一状态；
- 首选爬山法（First-choice hill-climbing）是随机生成后继节点，直到生成一个优于当前节点的后继；

### 模拟退火

我们总结提到的搜索方法：

- 爬山法：有效，但容易困在局部最优；
- 随机搜索：能找到全局最优，但低效.

因此提出模拟退火算法，它是由冶金工艺的“退火”启发得来：

![image-20211109192956465](/img/hsae/image-20211109192956465.png)

模拟退火算法比普通的爬山法多出来一步：以一定概率$(e^{\Delta E/T})$接受一个更差的邻居状态。这里的参数$T$被算法设计者形象化为温度，它被初始化为一个很大的值，然后逐渐减小至0.

### 局部束搜索

局部束搜索(Local Beam Search)维持了$k$个状态，初始化会随机生成$k$个状态，对于每次迭代，生成这$k$个状态的所有邻居，然后筛选出最优的$k$个。显然它只适用于离散状态问题。

### 连续空间下的搜索

常见的算法有梯度下降法与牛顿法，可以参考我写的<https://welts.xyz/2021/08/18/iteralgo/>。

### 演化算法

演化算法是一种受到进化论启发的算法，将问题的解作为种群，对其进行筛选（自然选择），重构（基因突变与基因重组），不断迭代这一过程，使其形成新的种群，最终得到更优的解：

![image-20211109200219317](/img/hsae/evolution.png)

这里只是概念的介绍，我们会在之后详细地分析演化算法。演化算法的特点：

- 易于并行；
- 很好地避免陷入局部最优；
- 适用于很多问题，只需要一种衡量解与解间优劣的度量；
- 不高效，但有加速空间。

我们在这里介绍了多种搜索/优化算法，除了演化算法之外都属于局部搜索(Local Search)，而演化算法是我们后面的重点研究内容。

## 演化算法(EA)

### 算法框架

演化算法的核心步骤就是[上面](#演化算法)这张流程图，在这里我们会逐步介绍图中操作的含义与方法。

#### 解的表示(representation)

演化算法中，种群是解(Solution)的集合，我们如何表示这些解？对于简单的数值优化问题

$$
\arg\max_{x}f(x)
$$

种群就是实数的集合；而对于一些搜索问题，比如八皇后问题：

![8queen](/img/hsae/image-20211109155232142.png)

我们不可能将一个棋盘的信息作为一个解（比如一个$8\times 8$）的矩阵，最简单的方式：用一个长度为8的数组记录从左到右八个棋子所在的行数。上图Solution对应的就是向量

$$
\begin{bmatrix}
4&3&2&5&4&3&2&3
\end{bmatrix}
$$

在计算机中，常常会用01串表示：

$$
\begin{bmatrix}
0100&0011&0010&0101&0100&0011&0010&0011
\end{bmatrix}
$$

在演化算法中，我们用"phenotype"来表示解在原始问题中的样子，用"genotype"来表示对解编码后的样子。在八皇后问题中，一个解的phenotype就是一个棋盘以及其上的八个皇后棋，genotype则是上述的向量表示，或二进制串表示。

#### 合适度(fitness)

根据进化论的观点，适者生存；在演化算法中，我们也需要对种群个体（一个解）衡量它对于当前问题的适合程度。比如在优化问题

$$
\arg\max_{x} x^2
$$

中，我们会用$x^2$来衡量一个解$x$的适合程度(fitness)：显然$x^2$越大，说明该解更优；在八皇后问题中，我们会用皇后之间发生冲突的对数来衡量解的优度：冲突对数越少，说明该解更优。

#### 种群(population)

种群很好理解，它维持着一系列的解作为候选解，一个genotype的集合。引出两个概念：

- 种群大小(Size of population)：种群中genotype的数量；
- 种群多样性(Diversity of population)：种群中不同fitness/phnotype/genotype的数量.

#### 初始化(initialization)

作为遗传算法的开始，我们会初始化一个种群，可以选择随机初始化的方式，也可以多次初始化，筛选出较优的解：

![init](/img/hsae/init.png)

可以发现，随着随机初始化次数的增加，会有越来越优的初始解生成。

#### 父代选择(parent selection)

对于当前种群，我们只会选择其中一部分进行后面的变异操作。显然，我们更愿意留下fitness更大的个体。我们通常是用概率的方法进行筛选：更高fitness的解有更大的被选择可能，但即使这个解再差，我们也不会一定不选择它（即筛选概率不为0）。

比如我们现在有三个解：

$$
\text{fitness}(A)=3\\
\text{fitness}(B)=1\\
\text{fitness}(C)=2\\
$$

那么$A$会有$\frac{3}{3+1+2}=50\%$的概率被选择，$B$有$17\%$的概率被选择，选择$C$的概率为$33\%$。

#### 变异(variation)

通过变异，我们可以产生新的解，也就是子代(offspring)解，演化算法中的变异操有两种：

- 突变(mutation)，即在父代解上随机产生很小的变化。比如父代解是

  $$
    \begin{bmatrix}
    1&0&1&1&1&0&0&0
    \end{bmatrix}
  $$

  第5位发生突变：

  $$
  \begin{bmatrix}
    1&0&1&1&0&0&0&0
  \end{bmatrix}
  $$
  
  类比生物学中的基因突变：DNA上的一个碱基发生变化；
- 重组(recombination/crossover)：将解与解的信息进行融合，形成新的子代解，比如两个父代解：

  $$
    \begin{bmatrix}
        1&0&1&1&1&0&0&0
    \end{bmatrix}\\
    \begin{bmatrix}
        0&0&1&0&1&0&1&0
    \end{bmatrix}
  $$

  基于第6位进行重组，形成两个子代解：

  $$
    \begin{bmatrix}
        1&0&1&1&1&0&1&0
    \end{bmatrix}\\
    \begin{bmatrix}
        0&0&1&0&1&0&0&0
    \end{bmatrix}\\
  $$

#### 幸存筛选(survivor selection)

到这里，种群有一些父代解和子代解，我们需要对其进行筛选，形成下一代种群。这里我们有两种筛选标准：

1. fitness标准：选择fitness高的解，淘汰fitness低的解；
2. age标准：选择年龄(存在时间)小的解，淘汰年龄大的解.

比如父代解A,B,C形成了子代解D,E,F, 其fitness如下：

$$
\text{fitness}(A)=3\\
\text{fitness}(B)=1\\
\text{fitness}(C)=2\\
\text{fitness}(D)=4\\
\text{fitness}(E)=1.5\\
\text{fitness}(F)=1\\
$$

以fitness为标准，我们会选择A,C,D；而以age为标准，我们会选择D,E,F。

#### 停止准则(stop criterion)

演化算法属于迭代算法，因此必须设置停止准则，它可以是

- 种群中存在到达预定fitness的解；
- 种群中解的个数到达某一最大值；
- 解在指定数代内没有fitness的提升；
- 种群多样性足够低.

#### 例子

假设我们要求解问题

$$
\arg\max_{x\in\{0,1,\cdots,31\}}x^2
$$

首先考虑representation，我们可以将这32个整数编码成长度为5的二进制向量，比如$x=15$等价于01串01111。

我们随机初始化了四个解作为初始种群：

![init](/img/hsae/init_pop.png)

然后进行重组与突变形成子代解：

![variation](/img/hsae/variation.png)

此时种群有8个解，我们这里使用age标准淘汰掉父代解，将剩下的4个个体投入下一次迭代。当然，我们也可以选择fitness标准。

我们打算用Python实现这个例子：

```python
from random import randint, uniform
from numpy.random import choice
'''
演化算法求解
    argmax x^2
    x∈{0,1,2,...,31}
'''

# 辅助函数
def fitness(x):
    '''对解的评估函数'''
    return x**2


def random_select(prob):
    '''
    模拟"以p的概率选择"
    '''
    if uniform(0, 1) <= prob:
        return True
    return False


def std_bin(n):
    '''
    将解转换成该问题中标准二进制，这里是5位
    '''
    result = bin(n)[2:]
    return "0" * (5 - len(result)) + result


def cross_over(pop1, pop2, position):
    '''
    对两个解进行交叉
    '''
    bin1, bin2 = std_bin(pop1), std_bin(pop2)
    return (
        eval("0b" + bin1[:position] + bin2[position:]),
        eval("0b" + bin2[:position] + bin1[position:]),
    )


def assign_str(s, n, c):
    '''
    通过下标修改字符串的内容，这里是突变需要
    '''
    l = list(s)
    l[n] = c
    return "".join(l)

# 种群大小，迭代次数，重组率，突变率
n_pop = 4
n_generation = 50
cross_over_rate = 0.25
mutation_rate = 0.25

# 随机初始化种群
population = [randint(0, 31) for i in range(n_pop)]

for n in range(n_generation):
    # parent selection：从父代解依概率筛选待变异子代
    fitness_list = [fitness(pop) for pop in population]
    prob_list = [fit / sum(fitness_list) for fit in fitness_list]

    offspring = []
    for i in range(len(population)):
        if random_select(prob_list[i]):
            offspring.append(population[i])

    # variation
    # cross over : 随机选择一对解进行交叉
    if len(offspring) >= 2 and random_select(cross_over_rate):
        i, j = choice(range(len(offspring)), 2, False)
        position = randint(0, 4)
        offspring[i], offspring[j] = cross_over(
            offspring[i],
            offspring[j],
            position,
        )
    # mutation : 每一个解都有一定概率发生随机突变
    for i in range(len(offspring)):
        if random_select(mutation_rate):
            position = randint(0, 4)
            bits = std_bin(offspring[i])
            bit = int(bits[position])
            bits = assign_str(bits, position, str(1 - bit))
            offspring[i] = eval("0b" + bits)

    # survivor selection : 根据fitness选择活下来的种群
    population = sorted(
        population + offspring,
        key=lambda x: fitness(x),
    )[-n_pop:]
```

我们可以在每次迭代中计算种群中的最优解：

```python
best_solu = max([fitness(x) for x in population])
```

然后将best fitness与迭代轮数的图像绘制出来：

![plot](/img/hsae/example_plot.png)

可以发现种群最优fitness最终能到达961，也就是最优解。

### 解表示、突变和重组

我们在前面对演化算法的各步骤与各要素进行了一个大致的解释，并编写了定义域为有限集下求解函数极值的程序。

在解释种群中解的表示(representation)时，我们只是介绍了简单的二进制表示，而在后面介绍变异时，我们也是针对二进制串进行演示。但现实问题是复杂的，不是每一种问题的解都可以用等长的二进制串来表示。所以本文的目的是介绍演化算法中常见的几种解的表示，以及对应的突变和重组方式。

#### 二进制表示(Binary representation)

在之前的八皇后问题，以及求解

$$
\mathop{\arg\max}\limits_{x\in\{0,1,\cdots,31\}}x^2
$$

时，我们将解表示为01串。当然二进制串不仅仅适用于这两种问题，比如背包问题(Knapsack problem)，对于$n$个物品，一个解$x$应该满足

$$
x\in\{0,1\}^n
$$

其中$x_i=1$表示背包中被装载第$i$个物品。

传统的二进制编码将整数直接转换成对应的二进制数；此外，格雷码是另外一种01编码：

![gray](/img/hsae/gray.png)

格雷码中任意两个相邻的代码只有一位二进制数不同。这样，如果解发生突变，突变后的子代解有很大概率与父代解相邻，控制了解的稳定性。

##### 二进制表示下的突变

对于每一个待突变的二进制表示的父代解，我们可以采用bit-wise突变：二进制串的每一位都有$p_m$的概率发生翻转。比如解发生了这样的突变

$$
\begin{bmatrix}
1&0&1&1&1
\end{bmatrix}\to\begin{bmatrix}
0&0&1&1&0
\end{bmatrix}
$$

也就是第1位和第5位发生了变异，则概率是$p_m^2(1-p_m)^3$。我们通常设$p_m=\frac{1}{n}$，$n$为01串的长度。设随机变量$X$表示变异的位数，则有

$$
P(X=k)={n\choose k}p_m^k(1-p_m)^{n-k}
$$

显然$X$服从二项分布$B(n,p_m)$.

One-bit突变是另一种突变方式：任选01串中的一位进行翻转（突变）。那么变异位数恒为1。

##### 二进制表示下的重组

One-point crossover随机选择一个点$r\in\\{1,2,\cdots,n-1\\}$，然后基于这个点，将两个父代解分割，然后通过交换尾部来创造两个子代解：

![cross_over](/img/hsae/cross_over.png)

形成概率为$\frac{1}{n-1}$。

m-point crossover会在01串上选择$m$个点，然后将两个子代解基于这些位置进行分割，然后交叉拼接，形成子代解，比如2-point crossover：

![m-point](/img/hsae/m_crossover.png)

显然出现上图子代解的概率：

$$
{n-1\choose 2}=\frac{2}{(n-1)(n-2)}
$$

Uniform crossover从另一种视角来看待重组：子代解1的第$i$位以$p$的概率是父代解1第$i$位，否则来自于父代解2的第$i$位，子代解2恰好相反。比如下图的交叉：

![uniform](/img/hsae/uniform_crossover.png)

出现这样的子代解概率为$p^3(1-p)^{5}$，子代解1只有3位是从父代解1集成，其余5位都是从父代解2继承。

> 只有突变可以产生新的信息，只有重组可以综合两个父代解的信息。

#### 整数表示

解就是一个多维整数向量：$x\in\mathbb{N}^n$。

##### 整数表示下的突变

整数表示下的突变方案和二进制表示下相同，比如bit-wise等，不同的是突变方式：整数表示下，突变可以是重新赋一个随机值，或者是加上一个很小的随机扰动。

##### 整数表示下的重组

和二进制表示下的重组方法相同。

#### 实数表示

解的形式是一个多维实向量：$x\in\mathbb{R}^n$

##### 实数表示下的突变

突变就是向量的变化：

$$
\pmb x=(x_1,\cdots,x_n)\to\pmb x^\prime=(x_1^\prime,\cdots,x_n^\prime)
$$

其中$x_i,x_i^\prime$都是有界的：$[lb_i,ub_i]$。突变分为uniform的和nonuniform的：

- Uniform突变：对于每一个$x_i$，都有$p_m$的概率改变为一个从均匀分布$[lb_i,ub_i]$中取样的值：

  $$
    x_i^\prime=\begin{cases}
       x_i&\text{with prob }1-p_m\\
       U(lb_i,ub_i)&\text{otherwise}\\
    \end{cases}
  $$

- Nonuniform突变：对于每一个$x_i$，加上一个从高斯分布$N(0,\sigma^2)$：

  $$
  x_i^\prime=x_i+\delta,\delta\sim N(0,\sigma^2)
  $$

对于nonuniform突变，有多种变种：

1. 相同步长的不相关突变：指定一个$\sigma$，每一个分量的突变都遵从

   $$
   x_i^\prime=x_i+\cdot N_i(0,\sigma^2)
   $$

2. 不同步长的不相关突变：每一个分量都有其对应的$\sigma_i$：

   $$
   x_i^\prime=x_i+N_i(0,\sigma_i^2)
   $$

3. 相关突变：对于不相关突变，分量之间都是独立的，它们服从均值为向量$\pmb x$，协方差矩阵为对角阵的多维高斯分布，如果将协方差矩阵由对角阵替换为非对角矩阵，那么此时的突变被称作相关突变。

**自适应突变**：前面的nonuniform突变中，我们需要自己设置$\sigma$的值，但实际上，$\sigma$可以与问题的解共同进化(coevolve)。共同进化下，解的形式就变成了$(x_1,\cdots,x_n,\sigma)$，在一次演化中，$\sigma$变成了$\sigma^\prime$，那么突变服从

$$
x_i^\prime=x_i+\sigma^\prime N_i(0,1)
$$

> 之所以提出自适应，是因为在不同的条件下，不同的$\sigma$会有不同的表现。自适应也被利用在梯度下降法中，由于函数在不同位置有不同的梯度，因此人为设置一个恒定步长$\eta$是不可靠的，在自适应梯度下降中，优化器会根据函数的不同情况调整步长，以此达到更快到达局部最优的目的。

对于不同的nonuniform突变，我们有不同的自适应规则：

- 相同步长的不相关突变：每一次迭代下，自适应参数的变化满足

  $$
    \sigma^\prime=\sigma\cdot e^{\tau\cdot N(0,1)}
  $$

  其中参数$\tau\propto\frac{1}{\sqrt n}$

- 不同步长的不相关突变：此时的自适应解形式为

  $$
    (x_1,\cdots,x_n,\sigma_{1},\cdots,\sigma_n)
  $$

  自适应更新：

  $$
    \begin{aligned}
        \sigma_i^\prime&=\sigma_i+\cdot e^{\tau^\prime\cdot N(0,1)+\tau\cdot N_i(0,1)}\\x_
        i^\prime&=x_i+\sigma_i^\prime\cdot N_i(0,1)
    \end{aligned}
  $$

  其中参数$\tau^\prime\propto\frac{1}{\sqrt{2n}},\tau\propto\frac{1}{\sqrt{2\sqrt{n}}}$

- 相关突变：因为自适应参数为一个协方差矩阵（对称的），我们实际上只需要考虑$\frac{n(n+1)}{2}$个元素即可。因此自适应解的形式：

  $$
    (x_1,\cdots,x_n,\sigma_1,\cdots,\sigma_n,\alpha_1,\cdots,\alpha_{n(n-1)/2})
  $$

  更新规则：

  $$
    \begin{aligned}
    \sigma_i^\prime&=\sigma_i\cdot e^{\tau^\prime\cdot N(0,1)+\tau\cdot N_i(0,1)}\\
    \alpha_j^\prime&=\alpha_j+\beta\cdot N_j(0,1)\\
    \end{aligned}
  $$

  其中$\tau^\prime\propto\frac{1}{\sqrt{2n}},\tau\propto\frac{1}{\sqrt{2\sqrt{n}}}$.

#### 实数表示下的重组

- 离散重组(Discrete recombination)：和前面二进制表示下的重组方法是一样的，比如m-point重组，uniform重组。
- 算数重组(Uniform recombination)：取父代解的中间值作为子代：
  
  $$
  z_i=(1-\alpha)x_i+\alpha y_i,\alpha_i\in[0,1]
  $$

  单算数重组在长度为$n$的解向量中任选一个位置进行重组：

  ![single](/img/hsae/single_arithmetic.png)

  一般算数重组则是随机选择$k$个位置进行单算数重组：

  ![simple](/img/hsae/simple_arithmetic.png)

  全算数重组：将解向量中所有的位置都进行算数重组：

  ![whold](/img/hsae/whole_arithmetic.png)

- 混合重组(Blend recombination)：在更大的范围内构造子代：
  
  $$
  \begin{aligned}
      z_i
      &=(1-\gamma)x_i+\gamma y_i,\gamma=(1+2\alpha)u-\alpha,u\in[0,1]\\
      &=x_i+(y_i-x_i)(1+2\alpha)u-\alpha(y_i-x_i)\\
      &\sim U(x_i-\alpha d_i,y_i+\alpha d_i),d_i=y_i-x_i
  \end{aligned}
  $$

- 多父重组(Multi-parent recombination)，将多个父代解进行交叉，形成多个子代解。比如，对于$m$个父代解，我们会随机选择$m-1$个交叉点，然后沿着对角来重组形成$m$个子代解：

  ![multi](/img/hsae/multi_parent.png)

#### 排列表示

虽然形式和整数表示相似，但属于不同的逻辑。比如在八皇后问题中，下面的棋盘

![image-20211112105157700](/img/hsae/image-20211112105157700.png)

的整数表示为

$$
\begin{bmatrix}
1&6&2&5&7&4&8&3
\end{bmatrix}
$$

而在旅行商问题中，城市遍历的顺序也可以是

$$
\begin{bmatrix}
1&6&2&5&7&4&8&3
\end{bmatrix}
$$

但第一种是整数表示，但恰好是一个组合的形式；第二种是组合表示，这是由旅行商问题的限制决定的。

排列表示的形式是一个指定集合的排列，可以有两种编码方式：

1. 事件在第$i$位发生；
2. 第$i$件事件发生的位置。

比如采用第一种编码方式的排列表示

$$
\begin{bmatrix}
1&6&2&5&7&4&8&3
\end{bmatrix}
$$

事件1在第1位发生，事件2在第6位发生，事件3在第二位发生......；采用第二种编码方式，编码变成：

$$
\begin{bmatrix}
1&3&8&6&4&2&5&7
\end{bmatrix}
$$

但两者解码后含义等价。

##### 排列表示下的变异

排列表示下，突变不能随机，比如

$$
\begin{bmatrix}
1&6&2&5&7&4&8&3
\end{bmatrix}\to\begin{bmatrix}
1&4&2&5&7&4&7&3
\end{bmatrix}
$$

变异之后，新子代不再是一个排列了。因此，排列表示的突变需要新的方式：

- 交换(Swap mutation)；
- 插入(Insert mutation);
- 争夺(Scramble mutation);
- 逆转(Inversion mutation).

###### 交换变异

交换变异就是在排列中随机选两个位置，将元素进行交换，这样形成的还是一个排列：

$$
\begin{bmatrix}
1&\pmb6&2&5&\pmb7&4&8&3
\end{bmatrix}\to\begin{bmatrix}
1&\pmb7&2&5&\pmb6&4&8&3
\end{bmatrix}
$$

###### 插入变异

插入变异：随机选两个位置，将第二个位置的元素移到第一个元素的正后面：

$$
\begin{bmatrix}
1&\pmb6&2&5&\pmb7&4&8&3
\end{bmatrix}\to\begin{bmatrix}
1&\pmb6&\pmb7&2&5&4&8&3
\end{bmatrix}
$$

它保留了大部分顺序信息和邻近信息。

###### 争夺变异

随机选择位置的子集，然后随机重排：

$$
\begin{bmatrix}
1&\pmb6&2&5&\pmb7&\pmb4&\pmb8&3
\end{bmatrix}\to\begin{bmatrix}
1&\pmb8&2&5&\pmb6&\pmb7&\pmb4&3
\end{bmatrix}
$$

###### 逆转变异

随机选择两个位置，将这两位之间的元素全部逆转：

$$
\begin{bmatrix}
1&\pmb6&\pmb2&\pmb5&\pmb7&4&8&3
\end{bmatrix}\to\begin{bmatrix}
1&\pmb7&\pmb5&\pmb2&\pmb6&4&7&3
\end{bmatrix}
$$

##### 排列表示下的重组

显然，对于排列表示的重组也不能随机重组，因为生成的子代有概率不是排列。排列表示的重组方法：

- Partially mapped crossover;
- Edge crossover;
- Order crossover;
- Cycle crossover.

我们来一一介绍这些重组方法：

###### Partially mapped crossover

步骤如下(直接翻译)：

1. 随机选择两个交叉点，然后将父代解1中两点间的部分复制下给子代解1；
2. 从第一个交叉点开始，寻找父代解2在两点间元素中未被复制的部分；
3. 对于每一个$i$​，查看其后代，看看𝑗有什么元素是从父代解1那里复制过来的；
4. 把𝑖放在父代解2中的𝑗所占据的位置，因为我们知道不会把𝑗放在那里(因为已经在后代中)；
5. 如果父代解2中𝑗所占的位置已经被填满，把𝑖放在父代解2中𝑘的位置；
6. 在处理了交叉部分的元素之后，剩下的父代解2可以填充第一个后代；
7. 以父母角色相反的方式创造第二个后代；

举例，现在我们有待重组的父代解：

$$
\begin{bmatrix}
1&2&3&4&5&6&7&8&9\\
9&3&7&8&2&6&5&1&4
\end{bmatrix}
$$

1. 随机选择交叉点：第4位和第7位，子代解1（-1表示未确定元素）：

   $$
   \begin{bmatrix}
   -1&-1&-1&4&5&6&7&-1&-1\\
   \end{bmatrix}
   $$

2. 从第4位开始，找父代解2的第4位和第7位之间不同于父代解1在该区间的元素，这里是8、2、6、5，其中5和6与子代解1重复，因此选择8和2；

3. 找对应：父代解1中4对应父代解2中的8，而父代解2中的4在最后一位，因此子代的最后一位是8；父代解1中5对应父代解2中的2，而父代解2中的5在两个随机选择的交叉点内，因此找父代解2中的5与父代解1对应的元素，是7，其在父代解2中对应的是第3位，在交叉点框定范围之外，因此子代解1的第3位是2：

   $$
   \begin{bmatrix}
   -1&-1&2&4&5&6&7&-1&8\\
   \end{bmatrix}
   $$

4. 将父代解2剩余的元素填充子代解1：

   $$
   \begin{bmatrix}
   9&3&2&4&5&6&7&1&8\\
   \end{bmatrix}
   $$

5. 用同样的方式构造第二个子代。

过程图示如下：

<img src="/img/hsae/image-20211113105257416.png" alt="image-20211113105257416" style="zoom:67%;" />

###### Edge crossover

构建一个列表表格(table listing)，用来记录父代解中指定元素的邻居元素(edge)，理论上说会4个edge，但如果存在重复元素，我们会在后面标记一个"+"，举个例子，现在我们有待重组的父代解：

$$
\begin{bmatrix}
1&2&3&4&5&6&7&8&9\\
9&3&7&8&2&6&5&1&4
\end{bmatrix}
$$

然后就可以列出表格

| Element |  Edges  |
| :-----: | :-----: |
|    1    | 2,5,4,9 |
|    2    | 1,3,6,8 |
|    3    | 2,4,7,9 |
|    4    | 1,3,5,9 |
|    5    | 1,4,6+  |
|    6    | 2,5+,7  |
|    7    | 3,6,8+  |
|    8    | 2,7+,9  |
|    9    | 1,3,4,8 |

在构建出这样一表格之后，我们需要：

1. 选一个初始元素和entry，将其放入子代；
2. 设变量“当前元素”的值为entry；
3. 移除表中所有指向当前元素的元素；
4. 检查当前元素的列表：
   - 如果有公共边，选择它称为下一个元素；
   - 否则，选择有最短列表的元素作为entry;
   - Ties are split at random；
5. 如果形成了一个空列表，则随机再选一个元素。

以上面的父代解为例：

![image-20211113111033885](/img/image-20211113111033885.png)

###### Order crossover

1. 随机选择两个交叉点；
2. 将父代解1两个交叉点内的部分复制到子代解1的相同位置；
3. 将不在上述部分的元素复制到子代解1，遵从如下规则：
   - 从第二个交叉点开始；
   - 以父代解2的顺序；
   - 如果遍历到达尾部，跳转到头部，继续遍历.
4. 用相同的方式创建子代解2，只不过两父代解角色互换。

还是以上面的父代解为例，选择第4位和第7位，父代解1就是

$$
\begin{bmatrix}
-1&-1&-1&4&5&6&7&-1&-1\\
\end{bmatrix}
$$

然后从7的后面开始，元素排列按照父代解2的顺序，也就是：

$$
1\to(4)\to9\to3\to(7)\to8\to2\to(6)\to(5)
$$

注意：

1. 上面有括号的元素已经出现在了子代解1中，所以需要跳过；
2. 在4之后，遍历到达尾部，此时要跳转到头部，也就是9，继续遍历。

因此子代解1为

$$
\begin{bmatrix}
3&8&2&4&5&6&7&1&9\\
\end{bmatrix}
$$

子代解2按照类似的方式构造，只是父代解1和2角色互换。

###### Cycle crossover

为什么叫“Cycle”，因为该方法就是在两个父代解序列中，利用等位基因找一个圈。“等位基因”在这里就是两父代解中位置相同的元素。具体步骤如下：

1. 从父代解1的第一个未被使用的位置开始；
2. 找父代解2中的等位基因；
3. 将等位基因的元素作为位置，对父代解1进行寻址；
4. 将这个等位基因加入圈；
5. 重复2-4步直到回到了第一步所说的位置；
6. 通过选择交替的圈来从每个父代创造后代.

举例：

![image-20211113114643122](/img/image-20211113114643122.png)

Partially mapped crossover和Edge crossover是将两个父代解的邻近信息赋给了子代；而Order crossover和Cycle crossover是将父代解的顺序信息给了子代。

#### 树表示

树形结构的解表示，可用于程序设计、数学表达式、逻辑表达式等场景。比如

```c
i = 1;
while (i < 20) {
    i = i + 1;
}
```

对应的树形式可以是

<img src="/img/hsae/image-20211113115924059.png" alt="image-20211113115924059" style="zoom:67%;" />

再比如数学表达式

$$
2\cdot\pi+\bigg((x+3)-\frac{y}{5+1}\bigg)
$$

就可以写成

<img src="/img/hsae/image-20211113120042079.png" alt="image-20211113120042079" style="zoom:67%;" />

逻辑表达式也可以采用树形式，比如逻辑表达式

$$
(x\wedge\text{True})\to((x\vee y)\vee(z\leftrightarrow(x\wedge y)))
$$

对应树表示：

![image-20211113120413605](/img/image-20211113120413605.png)

##### 树表示的突变

树的突变就是随机选择一个子树替换成一个随机生成的树：

<img src="/img/hsae/image-20211113120600769.png" alt="image-20211113120600769" style="zoom:80%;" />

##### 树表示的重组

树表示下的重组，就是在两棵父代树中各自随机选择两棵，然后互相交换：

<img src="/img/hsae/image-20211113132821507.png" alt="image-20211113132821507" style="zoom:80%;" />

#### 关于突变与重组

到底两者中哪个更好？这一争论持续了几十年：演化编程(Evolutinary programming)没有重组的概念，而基因编程(Genetic programming)是没有突变的。重组是具有探索性的，可以带来很大的飞跃；突变是具有利用性的，它创造了很小的差异。在现代，我们通常会将两者都纳入到演化算法中。

### 筛选与多样性

#### 父辈选择

我们前面提到过，在parent selection步骤，我们需要在种群中选择一部分个体进行变异。我们通常会根据解的fitness大小来决定是否对其进行变异。

##### Fitness proportional selection

Fitness proportional selection(FPS)用fitness的相对大小来决定选择其参与变异的概率：

$$
\Pr_{FPS}(i)=\dfrac{f_i}{\sum_{j=1}^\mu f_j}
$$

其中$f_i$表示第$i$个解的fitness，$\mu$是种群的大小。但当各个解的fitness很接近时，几乎没有选择压力(selection pressure)，也就是无法体现解的优劣程度。这时，我们可以对fitness进行处理(Windowing)：

$$
f_i=f_i-\beta^t\\
\beta^t=\min_{j\in\{1,2,\cdots,\mu\}}f_j
$$

这样能够增大解之间的差异，从而更好进行筛选。

##### Ranking selection

Ranking selection(RS)也是一种parent selection方法。RS的选择概率是基于相对的fitness而不是绝对的fitness。给每一个解一个选择概率，$0\leq rank\leq\mu-1$。Linear ranking selection(LRS)用一种线性的rank来衡量fitness：

$$
\Pr_{LRS}(i)=\dfrac{2-s}{\mu}+\frac{2i(s-1)}{\mu(\mu-1)}
$$

这样设计保证了概率和为1，符合概率公理：

$$
\sum_{i=0}^{\mu-1}\Pr_{LRS}(i)=\frac{2-s}{\mu}\cdot\mu+\frac{2(s-1)}{\mu(\mu-1)}\frac{\mu(\mu-1)}{2}=1
$$

从公式中可以看出，最差个体的选择概率为$\frac{2-s}{\mu}$。对于参数$s$，允许的合理范围是$s\in(1,2]$，它表示在使用LRS方法筛选个体$\mu$次之后，选择出的最优个体的期望数。当$s$增大，**更优个体的选择概率更大，更差个体的选择概率更小**。当$\mu$是奇数，那么选择fitness为中位数的个体的概率是一个**常数**：$\frac1\mu$。下面是LRS在不同$s$下，选择不同fitness个体的概率：

| 个体 | fitness | rank | $s=1.5$ | $s=2$ |
| :--: | :-----: | :--: | :-----: | :---: |
|  A   |    1    |  0   |   0.1   |   0   |
|  B   |    4    |  1   |  0.15   |  0.1  |
|  C   |    5    |  2   |   0.2   |  0.2  |
|  D   |    7    |  3   |  0.25   |  0.3  |
|  E   |    9    |  4   |   0.3   |  0.4  |

除了线性的ranking selection，我们还可以引入指数的ranking selection(Exponential ranking selection, ERS)：

$$
\Pr_{ERS}(i)=\dfrac{1-e^{-i}}{c}
$$

其中$C$是归一化系数，因为概率的和必须为1：

$$
\sum_{i=0}^{\mu-1}\dfrac{1-e^{-i}}{c}=1\to c=\mu-\dfrac{1-e^{-\mu}}{1-e^{-1}}
$$

#### 采样

##### Roulette wheel

在设计好每个个体被选择的概率之后，我们就可以设计采样方法，一种简单的方法就是Roulette wheel方法，模拟赌场里面的旋转的轮盘。假设种群有6个个体，对应的轮盘就是

<img src="/img/hsae/image-20211113154338036.png" alt="image-20211113154338036" style="zoom:67%;" />

每一个扇形的弧度对应着选取某个体的概率。$a_i=\sum_{j=1}^i\Pr_{sel}(j)$。然后从均匀分布$U[0,1]$中选出一个数$r$，观察$r$落在的位置，比如上图，就相当于我们抽中了第五个个体。再举个例子，假设种群有3个个体，筛选概率分别是0.1,0.2,0.3,0.4以此将$[0,1]$区间分成四段，长度比就是概率比，从0~1随机选一个数字$r$，$r$落在哪个区间，就要筛选哪个样本，比如$r=0.4$，$r$在$0.1+0.2$和$0.1+0.2+0.3$之间，因此筛选第3个个体。

假设我们要选择$\lambda$个解去变异，理论上第$j$个个体在待变异种群(也叫做mating pool)中的个数为

$$
\lambda\cdot\Pr_{sel}(j)
$$

但实际结果（采样）往往和理论值（期望）有很大不同，我们需要一种新方法来使得理论值和实际值足够接近。

##### Stochastic universal sampling

由此引入随机遍历抽样(Stochastic universal sampling)：假设要选择$\lambda$个个体，先从均匀分布$U[0,\frac1\lambda]$中选出$r$，然后将$r,r+\frac1\lambda,r+\frac2\lambda,\cdots,r+\frac{\lambda-1}{\lambda}$所在的轮盘区域对应个体选出来作为mating pool.

此时，如果$\lambda\cdot\Pr_{sel}(j)$是整数，那么mating pool中必存在这么多的个体$j$；即使不是整数，那也是$\lfloor\lambda\cdot\Pr_{sel}(j)\rfloor$或$\lfloor\lambda\cdot\Pr_{sel}(j)\rfloor+1$，与期望十分接近。

竞标赛选择法(Tournament selection, TS)只使用局部fitness信息：

- 随机选择$k$个个体，可以带重复，也可以不带重复；
- 比较这$k$个个体的fitness，选择最优的个体；
- 带放回地循环上面的过程$\lambda$次.

假设我们选择的时候允许重复，然后最优解唯一，那么被选出来的$\lambda$个个体中最少有一个最优解的概率为：

$$
1-\bigg(
1-{\mu-1\choose k-1}\bigg/{\mu\choose k}
\bigg)^\lambda=1-(1-\frac{k}{\mu})^\lambda
$$

##### Uniform selection

均匀选择(Uniform selection, US)从当前种群中随机选择个体：

$$
\Pr_{US}(i)=\frac1\mu
$$

也就是带放回的抽样。

#### 幸存筛选

在经历了parent selection和变异之后，当前种群的组成就是$\mu$个父代解和$\lambda$个子代解。我们需要通过survivor selection缩小种群，重新回到$\mu$个个体。

##### Age-based replacement

我们之前也提到过，有两种基于不同指标的淘汰方式：基于age和基于fitness。先来看基于年龄的淘汰：如果$\lambda=\mu$，我们可以直接淘汰所有的父代解，保留所有的子代解；如果$\lambda<\mu$，我们会淘汰掉年龄最大的$\lambda$个父代解，保持种群大小维持在$\mu$。

##### Fitness-based replacement

基于fitness的淘汰：如果$\mu>\lambda$，那么我们会淘汰父代解中fitness最小的$\lambda$个个体；如果$\mu<\lambda$，我们会在子代解中选出fitness最大的$\mu$个个体作为下一代；但我们也可以将父代和子代综合考虑，也就是$(\mu+\lambda)$选择：从$\mu+\lambda$个个体中选出fitness最大的$\mu$个个体作为下一代。

Round-robin tournament法：对$(\mu+\lambda)$中每一个个体$x$，都从种群中随机选出$q$个其他个体作为对手比较fitness，fitness大的则是赢家，记录$x$赢的次数，然后我们选出赢的次数最多的$\mu$个个体作为我们的新种群。

> 当$q=\mu+\lambda-1$时，算法退化成$(\mu+\lambda)$选择；当$q=1$时，即使最坏的解也可能被选中。

#### 种群多样性

父代筛选(Parent selection)和幸存筛选(Survivor selection)都会让演化算法专注于一个“峰”，因此存在陷入局部最优的可能：

![image-20211113170156063](/img/hsae/image-20211113170156063.png)

##### Fitness sharing

因此，我们想让种群集中在多个峰，类比到生物学概念，就是维持种群内部的多样性。其中一个方法是fitness sharing，通过共享解的fitness，从而限制一个“峰”上的个体数：

$$
f'(i)=\dfrac{f(i)}{\sum_{i}sh(d(i,j))}
$$

其中

$$
sh(d)=\begin{cases}
1-(\frac{d}{\delta_{share}})^\alpha&\text{if }d\leq\delta_{share}\\
0&\text{otherwise}
\end{cases}
$$

观察上式，如果有多个个体集中在某处，而且彼此间距离很小(d很小)，那么$sh(d)$就会很大，进而这些个体的fitness都会有一定程度的减小，如果是基于fitness的幸存筛选，这些个体被淘汰的可能性增加，从而避免了多个个体聚集在一个“峰”上。

##### Crowding

Crowding的思想是：子代解只会和相似的父代解产生竞争关系并排挤它们。比如下面的父代解$p_1,p_2$各自产生了子代解$o_1,o_2$，那么就有左边的距离关系：

![image-20211113172644351](/img/hsae/image-20211113172644351.png)

类似的，如果$p_1$产生了$o_2$，$p_2$产生了$o_1$，那么就有距离关系：

![image-20211113172734600](/img/hsae/image-20211113172734600.png)

因此，新生的子代解会将其附近的父代解顶替掉，维持了一个“峰”上的种群个体数。

![image-20211113172856897](/img/hsae/image-20211113172856897.png)

上图是采用不同的维持多样性方法而产生的结果，对于fitness sharing，越高的峰会有越多的个体，从而保持了各个不同高度峰对应个体的fitness的近似；而在Crowding中，只有子代替代父代的过程，没有fitness的参与，因此每个峰上个体数差不多。

### 演化算法的流行变种

我们之前几乎对每个步骤都介绍了几种方法，这就导致演化算法必然会有很多变种，我们这里对它们进行简单的介绍。

#### 遗传算法

遗传算法(Genetic Algorithms, GA)常用来解决离散域上的优化问题。Simple GA(SGA)作为其中的代表，有以下的特点：

- 解的表示：二进制表示；
- 重组方式：单点重组；
- 突变方式：bit-wise突变；
- 父代选择：利用Roulette wheel实现的fitness proportional selection；
- 幸存选择：基于age-based的淘汰，且$\lambda=\mu$，也就是子代全部替换父代.

遗传算法在集成学习中的应用[Zhou, 2012]：基础的集成学习是将多个基学习器的结果综合输出，但实际上，这些基学习器的效果有好有差，差学习器的存在将导致学习性能的下降。因此我们需要对这些基学习器进行选择(selective ensemble/ensemble pruning)，不仅有更好的性能，还可以减少内存的消耗，提升训练效率。

对于这个问题，我们有两个训练目标：

1. 最大化泛化性能；
2. 最小化基学习器数目.

使用uniform的父代选择，bit-wise突变和fitness-based幸存筛选的GA算法，用01串来表示解：$x_i=1$表示第$i$个基学习器被选中，$x_i=0$则没有。

#### 进化策略算法

进化策略算法(Evolutionary strategies, ES)应用于解决连续域上的优化问题。其特点与细节：

- 解的表示：实值表示；
- 重组方式：离散重组或算术重组(Discrete or arit特点：hmetic)；
- 突变方式：高斯摄动(Gaussian perturbation)；
- 父代选择：均匀随机(Uniform random)；
- 幸存选择：fitness-based，$(\mu,\lambda)$或$(\mu+\lambda)$式；
- 特点：自适应的突变参数(step size).
  
一个应用就是强化学习，比如双杆车问题：

![es](/img/hsae/es_example.png)

我们可以用ES算法与神经网络得到一个策略$\pi$，用于对模型进行调整。

#### 进化编程

进化编程(Evolutionary programming)最初是用来解决有限状态自动机的优化问题，现在被用在连续域上的优化，几乎和ES相融合，这里只介绍其与ES的不同点：

- 重组：没有重组过程；
- 父代选择：确定性的，每个父代解都会产生子代；
- 幸存选择：round-robin oturnament.

#### 遗传编程

遗传编程(Genetic probramming, GP)用于优化计算机程序，其特点：

- 解的表示：树表示；
- 重组方式：子树替换；
- 突变方式：树节点的随机替换；
- 父代选择：fitness proportional；
- 幸存选择：基于age-based的淘汰，且$\lambda=\mu$，也就是子代全部替换父代.

此外，GP在一次换代中，只会使用重组**或**突变，两者以一定概率发生一个；而对于GA，两种方法它都使用。

#### 差分演化

差分演化(Differential evolution, DE)用于解决非线性和不可微的连续优化问题。

- 解的表示：实值表示；
- 重组方式：均匀重组；
- 突变方式：**微分突变**(Differential mutation)；
- 父代选择：均匀随机；
- 幸存选择：一对一(parent vs. offspriing)的基于fitness筛选。

该算法一个特点是突变的方式：微分突变。它的步骤是：随机选择3个父代解$\pmb{x},\pmb{y},\pmb{z}$. 然后$\pmb x$的突变形式$\pmb v$为

$$
\pmb{v}=\pmb{x}+F\cdot(\pmb y-\pmb z)
$$

其中$F$是一个变异算子，通常在0-2之间。

#### 粒子群算法

粒子群算法(Partical swarm optimization, PSO)常用于非线性优化问题。

- 解的表示：实值表示；
- 重组方式：不采用重组；
- 突变方式：**加入速度向量(velocity vector)**；
- 父代选择：一对一，决定性的；
- 幸存选择：子代全部替代父代.

粒子群算法的每一个个体形式是一个元组：$(\pmb{x},\pmb{y})$，分别代表解向量和扰动向量(称作速度)，算法将这样的个体称作一个**粒子**。粒子的突变规则：

$$
\begin{aligned}
\pmb{v}^\prime&=w\cdot\pmb{v}+\phi_1\pmb{U}_1\cdot(\pmb y-\pmb x)+\phi_2\pmb{U}_2\cdot(\pmb{z}-\pmb{x})\\
\pmb{x}^\prime&=\pmb{x}+\pmb{v}^\prime
\end{aligned}
$$

我们来解释上式：$w$是代表惯性的权重，$\phi_1$表示“自身影响的学习率”，$\pmb U_1$是随机矩阵，$\pmb y$是**这个解**当前能到达的最优解；$\phi_2$表示“群体影响的学习率”，$\pmb U_2$也是一个随机矩阵，而$\pmb z$是当前种群中**所有解**能到达的最优解。然后依靠这些影响来更新解$\pmb{x}$。

#### 蚁群算法

蚁群算法(Ant colony optimization)用于在图中寻找较好的路径。该算法受启发于蚁群会使用信息素以帮助它们找到最优（最短）的路径。显然解的表示就是图上的路径。算法在每次迭代中实际上做了两件事：

1. 解的构建：一只蚂蚁在图上根据信息素与边的长度进行移动；
2. 信息素更新：每条边上的信息素根据经过它的蚁群数量和路径长度进行更新.

先来看解的构建：对于蚂蚁$k$，它当前位于节点$i$，选择$j$为其将要前往的节点概率：

$$
p_k(i,j)=\begin{cases}
\dfrac{\tau(i,j)^\alpha\eta(i,j)^\beta}{\sum_{u\in J_k(i)}\tau(i,u)^\alpha\eta(i,u)^\beta}&\text{if }j\in J_k(i)\\
0&\text{otherwise}
\end{cases}
$$

其中$J_k(i)$表示和节点$i$相邻且$k$可以到达的节点集；$\tau(i,j)$表示该路径上信息素含量；$\eta(i,j)$通常设为$1/d(i,j)$，其中$d(i,j)$为节点$i$与节点$j$之间的距离。

然后是信息素的更新：在蚁群构建路径之后，信息素的更新：

$$
\begin{aligned}
\tau(i,j)&=(1-\rho)\cdot\tau(i,j)+\sum_{k=1}^m\Delta\tau_k(i,j)\\
\Delta\tau_k(i,j)&=\begin{cases}
\dfrac{1}{C_k},&\text{if }(i,j)\in R^k\\
0,&\text{otherwise}
\end{cases}
\end{aligned}
$$

逐一解释参数：$\rho$是蒸发稀疏，因为现实环境中信息素会随着时间而蒸发流失；$m$是种群数；$\Delta\tau_k(i,j)$是蚂蚁$k$在路径$(i,j)$上留下的信息素的密度。$C_k$是蚂蚁$k$走过的路径长度。$R^k$是蚂蚁$k$走过的路。

#### 分布估计算法

分布估计算法(Estimation of Distribution Algorithms, EDA)用于解决多种优化问题。EDA通过建立和抽样有前景的候选解的显式概率模型来指导最优解的搜索。基本步骤：

1. 构建模型；
2. 模型采样，选出采样解中的fitness最优的子集；

不断重复上面的两步以迭代。

我们可以选择不同的概率模型：

- 单变量模型：$P(x_1)\cdot P(x_2)\cdots P(x_n)$；
- 二变量模型：$\prod_{i=1}^nP(x_i\vert pa_i)$；
- 多变量模型：贝叶斯网络.

## 演化算法的理论分析

## 演化算法与优化

### 多目标优化

### 带约束优化
