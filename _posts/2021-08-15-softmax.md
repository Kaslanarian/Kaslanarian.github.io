---
layout:     post
title:      Softmax和交叉熵
subtitle:   公式推导
date:       2021-08-15
author:     Welt Xing
header-img: img/genshin.jpg
catalog:    true
tags:
---
## <center>引言

周志华老师的《机器学习》的神经网络一章中，均方误差被作为损失函数，但在实际的分类问题中，我们常用交叉熵损失来作为目标函数，同时配合Softmax函数作为最后一层的输出来学习。本文将推导其反向传播公式，同时用自己设计的框架进行实验。

## <center>交叉熵损失和Softmax函数

### 交叉熵损失

交叉熵损失如下：

$$
L=-\dfrac{1}{l}\sum_{i=1}^l\sum_{c=1}^Cy_{ic}\log(p_{ic})
$$

其中$l$为样本数，$y_{ic}$是一个指示函数：如果$x_i$的类别为$c$，则返回1，否则为0；$p_{ic}$则是$x_i$为类别$c$​​的概率。如果要使用该损失函数，要将神经网络的输出视作概率，所以交叉熵常常和Sigmoid函数或Softmax函数一起出现。从这个视角看，最后一层其实是将神经网络的实值输出转换成概率输出，然后再和one-hot形式的样本一起计算损失。

### Softmax

我们先来看Softmax函数，其表达式如下：

$$
\text{Softmax}(x_i)=\dfrac{e^{x_i}}{\sum_{j=1}^ne^{x_j}}
$$

可以发现Softmax将输入标准化成概率输出，相比于简单的归一化：

$$
f(x_i)=\dfrac{x_i}{\sum_{j=1}^n{x_j}}
$$

Softmax利用指数函数拉大的输入元素之间的差异，从而使概率差异更显著。Softmax和Sigmoid函数都有下面的特性：

$$
f'(x)=f(x)\big(1-f(x)\big)
$$

带给我们编程实现上的方便。

### Softmax和Sigmoid

这里我们再回顾下Sigmoid函数：

$$
\text{Sigmoid}(x_i)=\dfrac{1}{1+e^{-x_i}}
$$

对于一个输入向量，Sigmoid函数作用下，各输出值其实是独立的。考虑Sigmoid函数最常用的场景，是在对率回归中：将测试数据输入训练好的函数$f$中：

$$
f(x)=\dfrac{1}{1+\exp(-w^\top x-b)}
$$

函数值象征该样本为正类的概率，$f(x)$​​​​​​距离1越近，说明其为正类的概率越大。再回到神经网络，如果输出端用Sigmoid函数处理，第$i$​​​​​个输出值$p_i$​​​​​是该样本属于第$i$​​​​​类的概率。如果Sigmoid处理后输出$[1,1,1]$​​​​​​​​，理解为神经网络估计该样本有100%的概率是0类，1类和2类，那么该样本同时有三种类别的特征，而不能归一化成每一个类都有$\frac13$的概率，两种说法是不同的。

Softmax则相反，当Softmax处理后输出$[\frac13,\frac13,\frac13]$​，那么就是说该样本有相同的概率属于0，1，2类。

## <center>函数求导

利用链式法则，我们只需要探究$\dfrac{\partial L}{\partial\hat{y}}$，之后的步骤和西瓜书上的过程相同。

$$
\begin{aligned}
\dfrac{\partial L}{\partial\hat{y}}
&=\dfrac{\partial}{\partial\hat{y}}-\sum_{c=1}^Cy_{c}\log(p_{c})\\
&=-\dfrac{\partial}{\partial\hat{y}}\sum_{c=1}^Cy_{c}\log(\hat{y}_{c})\\
&=\begin{bmatrix}
-\dfrac{y_1}{\hat{y}_1}&\cdots&-\dfrac{y_C}{\hat{y}_C}
\end{bmatrix}
\end{aligned}
$$

在Python中可以利用广播机制轻松实现求梯度。再来看Softmax函数，是一个向量对向量求导，生成的是一个矩阵：

$$
\begin{aligned}
\dfrac{\partial f_i}{\partial x_j}
&=\dfrac{\partial}{\partial x_j}\dfrac{e^{x_i}}{\sum_{k=1}^le^{x_k}}\\
\end{aligned}
$$

此时便需要分类讨论，当$i=j$​：

$$
\begin{aligned}
\dfrac{\partial f_i}{\partial x_i}
&=\dfrac{\partial}{\partial x_i}\dfrac{e^{x_i}}{\sum_{k=1}^le^{x_k}}\\
&=\dfrac{e^{x_i}(\sum_{k=1}^le^{x_k})-e^{x_i}\cdot e^{x_i}}{\big(\sum_{k=1}^le^{x_k}\big)^2}\\
&=f_i-f_i^2\\
&=f_i(1-f_i)
\end{aligned}
$$

如果$i\neq j$：

$$
\begin{aligned}
\dfrac{\partial f_i}{\partial x_j}
&=\dfrac{\partial}{\partial x_j}\dfrac{e^{x_i}}{\sum_{k=1}^le^{x_k}}\\
&=e^{x_i}\cdot \dfrac{\partial}{\partial x_j}\dfrac{1}{\sum_{k=1}^le^{x_k}}\\
&=-e^{x_i}\cdot\dfrac{e^{x_j}}{\big(\sum_{k=1}^le^{x_k}\big)^2}\\
&=-f_i\cdot f_j
\end{aligned}
$$

由此相比于Sigmoid下求导出一个对角矩阵，Softmax求导结果：

$$
\dfrac{\partial f}{\partial x}=\begin{bmatrix}
f_1(1-f_1)& -f_1\cdot f_2&\cdots&-f_1\cdot f_n\\
-f_1\cdot f_2& f_2\cdot(1-f_2)&\cdots&\vdots\\
\vdots&\vdots&\ddots&\vdots\\
-f_1\cdot f_n&-f_2\cdot f_n&\cdots&f_n\cdot(1-f_n)
\end{bmatrix}
$$
