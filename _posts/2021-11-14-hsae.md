---
layout:     post
title:      演化算法的流行变种
subtitle:   简述
date:       2021-11-13
author:     Welt Xing
header-img: img/hsae/hsae.png
catalog:    true
tags:
    - 演化算法
---

我们还是先来看演化算法的普遍过程图：

![process](/img/hsae/evolution.png)

我们之前几乎对每个步骤都介绍了几种方法，这就导致演化算法必然会有很多变种，我们这里对它们进行简单的介绍。

## <center>遗传算法

遗传算法(Genetic Algorithms, GA)常用来解决离散域上的优化问题。Simple GA(SGA)作为其中的代表，有以下的特点：

- 解的表示：二进制表示；
- 重组方式：单点重组；
- 突变方式：bit-wise突变；
- 父代选择：利用Roulette wheel实现的fitness proportional selection；
- 幸存选择：基于age-based的淘汰，且$\lambda=\mu$，也就是子代全部替换父代.

### 应用

遗传算法在集成学习中的应用[Zhou, 2012]：基础的集成学习是将多个基学习器的结果综合输出，但实际上，这些基学习器的效果有好有差，差学习器的存在将导致学习性能的下降。因此我们需要对这些基学习器进行选择(selective ensemble/ensemble pruning)，不仅有更好的性能，还可以减少内存的消耗，提升训练效率。

对于这个问题，我们有两个训练目标：

1. 最大化泛化性能；
2. 最小化基学习器数目.

使用uniform的父代选择，bit-wise突变和fitness-based幸存筛选的GA算法，用01串来表示解：$x_i=1$表示第$i$个基学习器被选中，$x_i=0$则没有。

## <center>进化策略算法

进化策略算法(Evolutionary strategies, ES)应用于解决连续域上的优化问题。其特点与细节：

- 解的表示：实值表示；
- 重组方式：离散重组或算术重组(Discrete or arit特点：hmetic)；
- 突变方式：高斯摄动(Gaussian perturbation)；
- 父代选择：均匀随机(Uniform random)；
- 幸存选择：fitness-based，$(\mu,\lambda)$或$(\mu+\lambda)$式；
- 特点：自适应的突变参数(step size).
  
一个应用就是强化学习，比如双杆车问题：

![es](/img/hsae/es_example.png)

我们可以用ES算法与神经网络得到一个策略$\pi$，用于对模型进行调整。

## <center>进化编程

进化编程(Evolutionary programming)最初是用来解决有限状态自动机的优化问题，现在被用在连续域上的优化，几乎和ES相融合，这里只介绍其与ES的不同点：

- 重组：没有重组过程；
- 父代选择：确定性的，每个父代解都会产生子代；
- 幸存选择：round-robin oturnament.

## <center>遗传编程

遗传编程(Genetic probramming, GP)用于优化计算机程序，其特点：

- 解的表示：树表示；
- 重组方式：子树替换；
- 突变方式：树节点的随机替换；
- 父代选择：fitness proportional；
- 幸存选择：基于age-based的淘汰，且$\lambda=\mu$，也就是子代全部替换父代.

此外，GP在一次换代中，只会使用重组**或**突变，两者以一定概率发生一个；而对于GA，两种方法它都使用。

## <center>差分演化

差分演化(Differential evolution, DE)用于解决非线性和不可微的连续优化问题。

- 解的表示：实值表示；
- 重组方式：均匀重组；
- 突变方式：**微分突变**(Differential mutation)；
- 父代选择：均匀随机；
- 幸存选择：一对一(parent vs. offspriing)的基于fitness筛选。

该算法一个特点是突变的方式：微分突变。它的步骤是：随机选择3个父代解$\pmb{x},\pmb{y},\pmb{z}$. 然后$\pmb x$的突变形式$\pmb v$为

$$
\pmb{v}=\pmb{x}+F\cdot(\pmb y-\pmb z)
$$

其中$F$是一个变异算子，通常在0-2之间。

## <center>粒子群算法

粒子群算法(Partical swarm optimization, PSO)常用于非线性优化问题。

- 解的表示：实值表示；
- 重组方式：不采用重组；
- 突变方式：**加入速度向量(velocity vector)**；
- 父代选择：一对一，决定性的；
- 幸存选择：子代全部替代父代.

粒子群算法的每一个个体形式是一个元组：$(\pmb{x},\pmb{y})$，分别代表解向量和扰动向量(称作速度)，算法将这样的个体称作一个**粒子**。粒子的突变规则：

$$
\begin{aligned}
\pmb{v}^\prime&=w\cdot\pmb{v}+\phi_1\pmb{U}_1\cdot(\pmb y-\pmb x)+\phi_2\pmb{U}_2\cdot(\pmb{z}-\pmb{x})\\
\pmb{x}^\prime&=\pmb{x}+\pmb{v}^\prime
\end{aligned}
$$

我们来解释上式：$w$是代表惯性的权重，$\phi_1$表示“自身影响的学习率”，$\pmb U_1$是随机矩阵，$\pmb y$是**这个解**当前能到达的最优解；$\phi_2$表示“群体影响的学习率”，$\pmb U_2$也是一个随机矩阵，而$\pmb z$是当前种群中**所有解**能到达的最优解。然后依靠这些影响来更新解$\pmb{x}$。

## <center>蚁群算法

蚁群算法(Ant colony optimization)用于在图中寻找较好的路径。该算法受启发于蚁群会使用信息素以帮助它们找到最优（最短）的路径。显然解的表示就是图上的路径。算法在每次迭代中实际上做了两件事：

1. 解的构建：一只蚂蚁在图上根据信息素与边的长度进行移动；
2. 信息素更新：每条边上的信息素根据经过它的蚁群数量和路径长度进行更新.

先来看解的构建：对于蚂蚁$k$，它当前位于节点$i$，选择$j$为其将要前往的节点概率：

$$
p_k(i,j)=\begin{cases}
\dfrac{\tau(i,j)^\alpha\eta(i,j)^\beta}{\sum_{u\in J_k(i)}\tau(i,u)^\alpha\eta(i,u)^\beta}&\text{if }j\in J_k(i)\\
0&\text{otherwise}
\end{cases}
$$

其中$J_k(i)$表示和节点$i$相邻且$k$可以到达的节点集；$\tau(i,j)$表示该路径上信息素含量；$\eta(i,j)$通常设为$1/d(i,j)$，其中$d(i,j)$为节点$i$与节点$j$之间的距离。

然后是信息素的更新：在蚁群构建路径之后，信息素的更新：

$$
\begin{aligned}
\tau(i,j)&=(1-\rho)\cdot\tau(i,j)+\sum_{k=1}^m\Delta\tau_k(i,j)\\
\Delta\tau_k(i,j)&=\begin{cases}
\dfrac{1}{C_k},&\text{if }(i,j)\in R^k\\
0,&\text{otherwise}
\end{cases}
\end{aligned}
$$

逐一解释参数：$\rho$是蒸发稀疏，因为现实环境中信息素会随着时间而蒸发流失；$m$是种群数；$\Delta\tau_k(i,j)$是蚂蚁$k$在路径$(i,j)$上留下的信息素的密度。$C_k$是蚂蚁$k$走过的路径长度。$R^k$是蚂蚁$k$走过的路。

## <center>分布估计算法

分布估计算法(Estimation of Distribution Algorithms, EDA)用于解决多种优化问题。EDA通过建立和抽样有前景的候选解的显式概率模型来指导最优解的搜索。基本步骤：

1. 构建模型；
2. 模型采样，选出采样解中的fitness最优的子集；

不断重复上面的两步以迭代。

我们可以选择不同的概率模型：

- 单变量模型：$P(x_1)\cdot P(x_2)\cdots P(x_n)$；
- 二变量模型：$\prod_{i=1}^nP(x_i\vert pa_i)$；
- 多变量模型：贝叶斯网络.

## <center>总结

我们在这里介绍了八种演化算法的常用变体，其中前四种出现时间较早，而后四种都是近年来出现的变体。
