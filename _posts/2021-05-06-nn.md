---
layout:     post
title:      单隐层神经网络的实现
subtitle:   推导与编码
date:       2021-05-06
author:     Welt Xing
header-img: img/nn_header.svg
catalog:    true
tags:
    - 机器学习
---

## 引言

一年前接触$\text{PyTorch}$的时候，便有了手动构建神经网络的想法，但囿于编程技术和机器学习知识的匮乏，无法将想法变成现实。在学习了《机器学习》的神经网络章节后，便打算将其用`Python`实现出来一个单隐层的神经网络.

## 神经网络简述

> 神经网络是由具有自适应的简单单元组成的广泛并行互联的网络，它的组织能够模拟生物神经对真实世界的物体所作出的交互反应 ——$\text{Kohonen},1988$.

一个典型的神经网络如下面所示：

![nn](/img/nn_header.svg)

其中每一个圆圈都是一个下面这样的$m$输入1输出的神经元：

![neuron](https://miro.medium.com/max/1050/1*T4ARzySpEQvEnr_9pc78pg.jpeg)

它将多输入进行线性组合，加以偏置，通过激活函数输出信号

$$
y=\varphi(\sum_{i=1}^m\omega_ix_i-b)
$$

## 编程时的弯路

在经过上面的神经网络的介绍后，我们一个简单的想法是先写一个`Neuron`类，用来模拟神经元的行为：

```python
class Neuron:
    '''
    The class is used to imitate the behavior of neuron
    '''
    def __init__(self, input:int):
        self.input = input
        self.weight = np.random.rand(1, input)
        self.bias = np.random.rand(1, 1)

    @staticmathod
    def ReLU(x):
        '''
        激活函数使用ReLU
        '''
        return x>0? x : np.zeros(x.shape)

    def output(x):
        '''
        计算输出
        '''
        return Neuron.ReLU(self.weight @ x.T - self.bias)
```

然后一层神经元`NeuronLayer`就是一个`Neuron`数组，神经网络`NeuronNetwork`就是神经元层的数组. 这种设计是可行的，但难在层与层之间的交互：上一层的信号传导到下一层的信号要考虑同步，更新等问题，难度过大，我们会提出更简单可靠的实现方法. 事实上，越形象的模型，实现起来越复杂，越抽象的模型，越能够用数学模型去模拟.

## 从矩阵视角看神经网络

为了便于书写和讨论，我们选择去实现一个$3$输入，$4$个隐层神经元和$2$输出的神经网络；此外，选用$\text{Sigmoid}$作为激活函数：

$$
\text{Sigmoid}(x)=\dfrac{1}{1+\exp(-x)}
$$

也就是下面的网络：

![342nn](/img/nn.svg)

该网络中的参数有哪些？我们从左往右看：

- 输入层到中间层的边上的权重，如果我们设连接输入层中第$i$个神经元和隐藏层中第$j$个神经元的边上的权重为$v_{ij}$，那么就可以将输入层到隐藏层上的边权用矩阵写出来：

$$
V=\begin{bmatrix}
v_{11}&v_{12}&v_{13}&v_{14}\\
v_{21}&v_{22}&v_{23}&v_{24}\\
v_{31}&v_{32}&v_{33}&v_{34}\\
\end{bmatrix}
$$

$v_{ij}$表示第$i$个神经元到第$j$个神经元的权重.

- 中间层的偏置：

$$
B=\begin{bmatrix}
b_1&b_2&b_3&b_4
\end{bmatrix}
$$

$b_i$表示中间层第$i$的神经元的偏置.

- 中间层到输出层的权重矩阵：

$$
W=\begin{bmatrix}
w_{11}&w_{12}\\
w_{21}&w_{22}\\
w_{31}&w_{32}\\
w_{41}&w_{42}\\
\end{bmatrix}
$$

- 输出层的偏置：

$$
\Gamma=\begin{bmatrix}
\gamma_1&\gamma_2
\end{bmatrix}
$$

从而对于输入$x_{1\times 3}$，我们得到中间层的输出：

$$
\text{hidden}_{1\times4}=\text{Sigmoid}(x_{1\times3}V_{3\times4}-B_{1\times4})
$$

> 此处的$\text{Sigmoid}$函数有广播性，即$\text{Sigmoid}([x_1,x_2])=[\text{Sigmoid}(x_1),\text{Sigmind}(x_2)]$

输出层的输出

$$
\text{output}_{1\times2}=\text{Sigmoid}(\text{hidden}_{1\times4}W_{4\times2}-\Gamma_{1\times2})
$$

到这里，我们已经可以实现一个没有学习能力的神经网络：

```python
import numpy as np

class NeuronNetwork:
    '''
    用于模拟单隐层的神经网络类
    '''
    def __init__(self, input:int, hidden:int, output:int):
        '''
        初始化网络参数
        '''
        # 神经网络的基本信息
        self.input = input
        self.hidden = hidden
        self.output = output

        # 网络参数的随机初始化
        self.V = np.random.rand(input, hidden)
        self.W = np.random.rand(hidden, output)
        self.B = np.random.rand(1, hidden)
        self.Gamma = np.random.rand(1, output)
    
    @staticmethod
    def Sigmoid(x):
        '''
        Sigmoid函数，属于静态类方法.
        '''
        return 1 / (1 + np.exp(-x))

    def network(self, x):
        '''
        获取输入向量在中间层和输出端的输出
        '''
        hidden = NeuronNetwork.Sigmoid(x @ self.V - self.B)
        output = NeuronNetwork.Sigmoid(hidden @ self.W - self.Gamma)
        return hidden, output
```

> 这里的`@`运算可以参考[这里](https://welts.xyz/2021/04/26/numpy_dim/#%E7%AC%AC%E4%BA%94%E8%BF%90%E7%AE%97%E5%90%91%E9%87%8F%E4%B9%98%E6%B3%95)

## 反向传播及其实现

接下来的任务就是给`NeuronNetwork`类赋予`train`方法，也就是训练（学习）的能力，神经网络的训练依赖于反向传播方法.

对于一个样本输入$(x_{1\times3},y_{1\times2})$，数据经过神经网络的输出为$\hat{y}_{1\times2}$，则网络在该样本数据上的均方误差为

$$
\text{err}(x)=\dfrac{1}{2}\sum_{i=1}^2(\hat{y}_i-y_i)^2=\dfrac{1}{2}\Vert\hat{y}-y\Vert^2
$$

我们的目标就是最小化均方误差，这里使用梯度下降进行逼近，以目标的负梯度方向进行调整. 以$w_{ij}$为例：

$$
\begin{aligned}
\dfrac{\partial\text{err}(x)}{\partial w_{ij}}
&=\dfrac{\partial\text{err}(x)}{\partial\hat{y}_j}\dfrac{\partial\hat{y}_j}{\partial w_{ij}}\\
&=\dfrac{\partial\text{err}(x)}{\partial\hat{y}_j}\dfrac{\partial\text{Sigmoid}(\text{hidden}_{1\times4}\cdot\begin{bmatrix}w_{1j}&\cdots w_{4j}
\end{bmatrix}^\top-\gamma_j)}{\partial w_{ij}}\\
&=\dfrac{\partial\text{err}(x)}{\partial\hat{y}_j}\dfrac{\partial\text{Sigmoid}(t)}{\partial t}\dfrac{\partial t}{\partial w_{ij}}\cdots t=\text{hidden}_{1\times4}\cdot\begin{bmatrix}w_{1j}&\cdots w_{4j}
\end{bmatrix}^\top-\gamma_j\\
&=(\hat{y}_j-y_j)[\hat{y}_j(1-\hat{y}_j)]\text{hidden}_i
\end{aligned}
$$

> 这里运用到$\text{Sigmoid}$函数的一个特性：$f'(x)=f(x)(1-f(x))$

令

$$g_j=\hat{y}_j(1-\hat{y}_j)(y_j-\hat{y}_j)$$

于是

$$\Delta w_{ij}=-\eta\dfrac{\partial\text{err}}{\partial w_{ij}}=\eta g_j\text{hidden}_i$$

类似的，有：

$$
\begin{aligned}
\Delta\gamma_j&=-\eta g_j\\
\Delta v_{ij}&=\eta e_jx_i\\
\Delta b_h&=-\eta e_h
\end{aligned}
$$

其中

$$
\begin{aligned}
e_h=\text{hidden}_h(1-\text{hidden}_h)\sum_{j=1}^lw_{hj}g_j
\end{aligned}
$$

反向传播便是借助上面的公式去更新梯度，从而下降到极小值的方法：

![bp](/img/BP.png)

由此，我们就可以为我们的$\text{NeuronNetwork}$类增加`train`方法：

```python
class NeuronNetwork:
    ...
    def train(self, train_x, train_y, eta=0.1, threshold=1e-5):
        '''
        train_x : 列表型数据，如[[1, 2], [3, 4]]
        train_y : 列表型数据，如[[0, 1], [1, 0]]，分别表示第1类和第0类
        '''
        self.train_x = np.array(train_x)
        self.train_y = np.array(train_y)
        data_len = self.train_x.shape[0]

        old_error = sum([
            self.error(self.train_x[k], self.train_y[k])
            for k in range(data_len)
        ]) / data_len

        while True:
            for k in range(data_len):
                x, y = self.train_x[k], self.train_y[k]
                hidden, output = self.network(x)
                # 获取梯度
                g = output * (1 - output) * (y - output)
                e = hidden * (1 - hidden) * (self.W @ g.T).T
                # 更新权值
                self.W += eta * hidden.T @ g
                self.Gamma -= eta * g
                self.V += eta * (e.T @ np.array([self.train_x[k]])).T
                self.B -= eta * e

            # 获取更新后误差
            new_error = sum([
                self.error(self.train_x[k], self.train_y[k])
                for k in range(data_len)
            ]) / data_len

            # 到达阈值后便停止
            if abs(old_error - new_error) <= threshold:
                break
            else:
                old_error = new_error
                # print(old_error) # 选择是否将误差输出
```

此外，我们还可以加入预测的功能，也就是选择输出层中输出值最大的神经元序数作为类别序数

```python
class NeuronNetwork:
    ...
    def predict(self, x):
        output = self.network(x)[1]
        c = np.argmax(output)
        prob = output[0][c]
        return np.argmax(output), prob
```

返回类别和相应神经元输出（近似于输出的概率，越大说明越神经网络肯定分类结果）.

## 效果测试

我们先用下面的自制数据集作为训练集，观察分类效果：

```python
neuron_train_x = [
    [3, 3],
    [1, 1],
    [4, 3],
    [2, 2],
    [5, 2],
    [1, 2],
    [4, 4],
    [3, 1],
    [4, 2],
    [1, 3],
]
neuron_train_y = [
    [0, 1],
    [1, 0],
    [0, 1],
    [1, 0],
    [0, 1],
    [1, 0],
    [0, 1],
    [1, 0],
    [0, 1],
    [1, 0],
]
```

将数据特征数定义为2，以便可视化表示，我们定义$y=[0,1]$为正类，$y=[1,0]$为负类：

![nn_train](/img/nn_train.svg)

我们期望神经网络能够将正负样本分开，下面的测试函数先使用上面的数据进行训练，然后将训练好的模型分别对$[0,5]\times[0,5]$中的点进行测试，并利用`seaborn`的`heatmap`.

```python
def test():
    NN = NeuronNetwork(2, 4, 2)
    NN.train(
        neuron_train_x,
        neuron_train_y,
        eta=0.1,
        threshold=1e-7,
    )
    p = np.array(
        [[NN.predict(np.array([i, j]))[0] for j in np.linspace(0, 5, 100)]
         for i in np.linspace(5, 0, 100)])
    sns.heatmap(p)
    plt.xticks([]) # 删去x轴刻度
    plt.yticks([]) # 删去y轴刻度

test()
```

训练结果如下，深色为负类，淡色为正类

![img](/img/nn_learn.svg)

可以发现，神经网络确实返回了一个**近似**线性的决策边界（虽然无法显式求得解析式）.

## 用神经网络解决更复杂的分类任务

在上面的测试中，数据小，而且过于简单（线性可分），简单的感知机模型就可以在更短时间求解出一个解释性强的模型，所以我们打算再制造更复杂的数据，检测我们的神经网络的性能.

```python
import numpy as np

n = 20
theta = np.linspace(0, 2 * pi, n)

pos_data = np.array([
    2 * np.cos(theta) + 0.5 * np.random.rand(n),
    2 * np.sin(theta) + 0.5 * np.random.rand(n),
])

neg_data = np.array([
    4 * np.cos(theta) + np.random.rand(n),
    4 * np.sin(theta) + np.random.rand(n),
])

train_x = np.append(pos_data.T, neg_data.T, axis=0)
train_y = np.append([[0, 1]] * n, [[1, 0]] * n, axis=0)
```

我们将数据可视化：

```python
plt.scatter(pos_data[0], pos_data[1])
plt.scatter(neg_data[0], neg_data[1])
plt.show()
```

可以发现正负样本各自按一个圆弧分布：

![nn_more](/img/nn_train1.svg)

我们还是像上面那样去训练：

```python
NN = NeuronNetwork(2, 4, 2)
NN.train(
    train_x,
    train_y,
    eta=0.1,
    threshold=1e-7,
)
```

然后让模型预测$[-5, 5]\times[-5,5]$上的点集：

```python
p = np.array(
    [[NN.predict(np.array([i, j]))[0] for j in np.linspace(-5, 5, 200)]
     for i in np.linspace(5, -5, 200)])
sns.heatmap(p, vmin=0, vmax=1)
plt.xticks([])
plt.yticks([])
```

效果如下图所示：

![result](/img/nn_learn1.svg)

也就是说，我们的神经网络可以根据样本形成一个弧形的决策边界，这是普通的感知机所做不到的！相比于使用核方法才能实现非线性分类的支持向量机模型，神经网络牺牲了可解释性换来模型复杂度的降低.

我们通过增加类别数来再次增加数据集复杂度：

```python
n = 20
theta = np.linspace(0, 2 * pi, n)

Adata = np.array([
    np.cos(theta) * (2 + np.random.rand(n)),
    np.sin(theta) * (2 + np.random.rand(n)),
])

Bdata = np.array([
    np.cos(theta) * (4 + np.random.rand(n)),
    np.sin(theta) * (4 + np.random.rand(n)),
])

Cdata = np.array([
    np.cos(theta) * (6 + np.random.rand(n)),
    np.sin(theta) * (6 + np.random.rand(n)),
])

train_x = np.append(Adata.T, Bdata.T, axis=0)
train_x = np.append(train_x, Cdata.T, axis=0)
train_y = np.append([[1, 0, 0]] * n, [[0, 1, 0]] * n, axis=0)
train_y = np.append(train_y, [[0, 0, 1]] * n, axis=0)
```

我们的数据大致按照三个圆弧分布：

![nn_train2](/img/nn_train2.svg)

训练，这次增加了中间层的神经元数目.

```python
NN = NeuronNetwork(2, 10, 3)
NN.train(
    train_x,
    train_y,
    eta=0.5,
    threshold=1e-7,
)
```

用模型对$[-7,7]\times[-7,7]$上的样本做随机预测：

```python
p = np.array(
    [[NN.predict(np.array([i, j]))[0] for j in np.linspace(-7, 7, 200)]
     for i in np.linspace(7, -7, 200)])
sns.heatmap(p, vmin=0, vmax=2)
plt.xticks([])
plt.yticks([])
plt.show()
```

![nn_preict](/img/nn_learn2.svg)

发现神经网络在面对复杂的多分类问题时依旧保持高性能. 我们将`predict`模块进行修改，将神经元的输出归一化为分类概率：越大说明神经网络越肯定预测结果.

```python
class NeuronNetwork:
    ...
    def predict(self, x):
        output = self.network(x)[1]
        c = np.argmax(output)
        prob = output[0][c] / sum(output[0])
        return c, prob
```

我们也可以将上面的分类预测结果进行可视化：

```python
p = np.array(
    [[NN.predict(np.array([i, j]))[1] for j in np.linspace(-7, 7, 200)]
    for i in np.linspace(7, -7, 200)
])
sns.heatmap(p, vmin=0, vmax=1)
plt.xticks([])
plt.yticks([])
plt.show()
```

结果如下：

![prob](/img/nn_prob.svg)

根据图右边的图例，我们知道分类效果是不错的：除去决策边界附近，其他地方的数据预测都有很高的置信度；在决策边界出现模糊，但也保证了置信度大都是$0.5$以上，而不是平均的$\dfrac{1}{3}$.

## 结语

我们已经见识到，神经网络在分类问题上的强大能力：你不知道发生了什么，但它确实做的很好. 此外，本文的模型和现在使用的神经网络存在区别：层数更大，而且使用$\text{ReLU}$代替$\text{Sigmoid}$作为激活函数：

![2actfunc](https://anshdaviddev.com/assets/img/slp-activation-function/sig-vs-relu.png?style=center)

两者其实并不是独立的：层数的增加意味着反向传播，也就是链式求导的复杂度增加，迫使我们寻找微分更加简单的函数作为激活函数，$\text{ReLU}$应运而生. 这也给我后面的工作带来启示.
