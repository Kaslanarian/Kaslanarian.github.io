---
layout:     post
title:      简单神经网络的实现
subtitle:   推导与编码
date:       2021-05-06
author:     Welt Xing
header-img: img/nn_header.svg
catalog:    true
tags:
    - 机器学习
    - 编程技术
---

## 引言

一年前接触$\text{PyTorch}$的时候，便有了手动构建神经网络的想法，但囿于编程技术和机器学习知识的匮乏，无法将想法变成现实。在学习了《机器学习》的神经网络章节后，便打算将其用`Python`实现出来一个单隐层的神经网络.

## 神经网络简述

> 神经网络是由具有自适应的简单单元组成的广泛并行互联的网络，它的组织能够模拟生物神经对真实世界的物体所作出的交互反应 ——$\text{Kohoen},1988$.

一个典型的神经网络如下面所示：

![nn](/img/nn_header.svg)

其中每一个圆圈都是一个下面这样的$m$输入1输出的神经元：

![neuron](https://miro.medium.com/max/1050/1*T4ARzySpEQvEnr_9pc78pg.jpeg)

它将多输入进行线性组合，加以偏置，通过激活函数输出信号

$$
y=\varphi(\sum_{i=1}^m\omega_ix_i-b)
$$

## 编程时的弯路

在经过上面的神经网络的介绍后，我们一个简单的想法是先写一个`Neuron`类，用来模拟神经元的行为：

```python
class Neuron:
    '''
    The class is used to imitate the behavior of neuron
    '''
    def __init__(self, input:int):
        self.input = input
        self.weight = np.random.rand(1, input)
        self.bias = np.random.rand(1, 1)

    @staticmathod
    def ReLU(x):
        '''
        激活函数使用ReLU
        '''
        return x>0? x : np.zeros(x.shape)

    def output(x):
        '''
        计算输出
        '''
        return Neuron.ReLU(self.weight @ x.T - self.bias)
```

然后一层神经元`NeuronLayer`就是一个`Neuron`数组，神经网络`NeuronNetwork`就是神经元层的数组. 这种设计是可行的，但难在层与层之间的交互：上一层的信号传导到下一层的信号要考虑同步，更新等问题，难度过大，我们会提出更简单可靠的实现方法. 事实上，越形象的模型，实现起来越复杂，越抽象的模型，越能够用数学模型去模拟.

## 从矩阵视角看神经网络

为了便于书写和讨论，我们选择去实现一个$3$输入，$4$个隐层神经元和$2$输出的神经网络；此外，选用$\text{Sigmoid}$作为激活函数：

$$
\text{Sigmoid}(x)=\dfrac{1}{1+\exp(-x)}
$$

也就是下面的网络：

![342nn](/img/nn.svg)

该网络中的参数有哪些？我们从左往右看：

- 输入层到中间层的边上的权重，如果我们设连接输入层中第$i$个神经元和隐藏层中第$j$个神经元的边上的权重为$v_{ij}$，那么就可以将输入层到隐藏层上的边权用矩阵写出来：

$$
V=\begin{bmatrix}
v_{11}&v_{12}&v_{13}&v_{14}\\
v_{21}&v_{22}&v_{23}&v_{24}\\
v_{31}&v_{32}&v_{33}&v_{34}\\
\end{bmatrix}
$$

$v_{ij}$表示第$i$个神经元到第$j$个神经元的权重.

- 中间层的偏置：

$$
B=\begin{bmatrix}
b_1&b_2&b_3&b_4
\end{bmatrix}
$$

$b_i$表示中间层第$i$的神经元的偏置.

- 中间层到输出层的权重矩阵：

$$
W=\begin{bmatrix}
w_{11}&w_{12}\\
w_{21}&w_{22}\\
w_{31}&w_{32}\\
w_{41}&w_{42}\\
\end{bmatrix}
$$

- 输出层的偏置：

$$
\Gamma=\begin{bmatrix}
\gamma_1&\gamma_2
\end{bmatrix}
$$

从而对于输入$x_{1\times 3}$，我们得到中间层的输出：

$$
\text{hidden}_{1\times4}=\text{Sigmoid}(x_{1\times3}V_{3\times4}-B_{1\times4})
$$

> 此处的$\text{Sigmoid}$函数有广播性，即$\text{Sigmoid}([x_1,x_2])=[\text{Sigmoid}(x_1),\text{Sigmind}(x_2)]$

输出层的输出

$$
\text{output}_{1\times2}=\text{Sigmoid}(\text{hidden}_{1\times4}W_{4\times2}-\Gamma_{1\times2})
$$

到这里，我们已经可以实现一个没有学习能力的神经网络：

```python
import numpy as np

class NeuronNetwork:
    '''
    用于模拟单隐层的神经网络类
    '''
    def __init__(self, input:int, hidden:int, output:int):
        '''
        初始化网络参数
        '''
        # 神经网络的基本信息
        self.input = input
        self.hidden = hidden
        self.output = output

        # 网络参数的随机初始化
        self.V = np.random.rand(input, hidden)
        self.W = np.random.rand(hidden, output)
        self.B = np.random.rand(1, hidden)
        self.Gamma = np.random.rand(1, output)
    
    @staticmethod
    def Sigmoid(x):
        '''
        Sigmoid函数，属于静态类方法.
        '''
        return 1 / (1 + np.exp(-x))

    def network(self, x):
        '''
        获取输入向量在输出端的输出
        '''
        hidden = NeuronNetwork.Sigmoid(x @ self.V - self.B)
        output = NeuronNetwork.Sigmoid(hidden @ self.W - self.Gamma)
        return output
```

> 这里的`@`运算可以参考[这里](https://welts.xyz/2021/04/26/numpy_dim/#%E7%AC%AC%E4%BA%94%E8%BF%90%E7%AE%97%E5%90%91%E9%87%8F%E4%B9%98%E6%B3%95)

## 反向传播及其实现

接下来的任务就是给`NeuronNetwork`类赋予`train`方法，也就是训练（学习）的能力，神经网络的训练依赖于反向传播方法.
