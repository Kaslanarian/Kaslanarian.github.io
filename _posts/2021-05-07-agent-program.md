---
layout:     post
title:      栅格世界的决策算法
subtitle:   实验报告
date:       2021-05-07
author:     Welt Xing
header-img: img/Agent/prog.jpg
catalog:    true
tags:
    - 智能系统
---

## 前言

本文是对《智能系统设计与应用》第一次编程作业中所涉及的知识点介绍，以及编程想法的记录.

## 实验目的

在$10\times10$的栅格世界中，分别应用值迭代，高斯-赛德尔值迭代和策略迭代算法，使得$\text{Agent}$在这个栅格世界的每一个格子（也就是一个状态）都能获取到状态效用并找到最优决策$\pi^\star(s)$.

## 关于栅格世界

栅格世界如下图所示：

![grid](/img/Agent/grid.png)

我们用$(x,y)$表示$\text{Agent}$位于栅格世界的第$x$行第$y$列，在这个世界中，有如下的规则：

1. 无论$\text{Agent}$在哪个格子，都可以向上、下、左、右移动；
2. 行动的效果是随机的，朝指定方向移动的概率为$0.7$，朝其余3个方向移动的概率各为$0.1$；
3. 如果与墙碰撞，则原地不动，且受到1的惩罚；
4. 进入指定格子（$(x,y)=(8,9),(x,y)=(3,8)$）后执行任意行动的奖赏分别为$+10,+3$,随后行动终止；
5. 进入指定格子（$(x,y)=(5,4),(x,y)=(8,4)$）后执行任一行动的奖赏分别为$-5,-10$.

每一个格子$(x,y)$，都有一个最佳行动集合和一个效用值，分别表示能够使后继状态的期望效用最大的行动集合，该状态的立即回报加上在下一个状态的期望折扣效用值.

最佳行动集合的定义：

$$
\pi^\star(s)=\argmax_{a\in\mathcal{A}(s)}\sum_{s'}P(s'\vert s,a)U(s')
$$

状态$s$的效用值定义：

$$
U(s)=R(s)+\gamma\max_{a\in\mathcal{A}(s)}\sum_{s'}P(s'\vert s,a)U(s')
$$

上式称作**效用的贝尔曼方程**。两式合并：

$$
U(s)=R(s,\pi^\star(s))+\gamma\sum_{s'}P(s'\vert\pi^\star(s),a)U(s')
$$

我们接下来的工作主要就是围绕上面的公式进行.

## 价值迭代算法

以我们的栅格世界为例，只要能够获取到每个格子对应的效用值，$\text{Agent}$的行动便被我们完全理解；但实际上，贝尔曼方程告诉我们，如果有$n$个可能的状态，就有$n$个贝尔曼方程，我们这里就是$100$个. 此外，贝尔曼方程因为$\max$算符的存在而成为非线性方程。我们的想法是用迭代法，从**任意**的初始效用值开始，算出方程的右边，再把它代入到左边，第$i$次迭代就是下面的操作：

$$
U_{i+1}(s)\gets R(s)+\gamma\max_{a\in\mathcal{A}(s)}\sum_{s'}P(s'\vert s,a)U_i(s')
$$

理论上（这里略去证明），不断的迭代会使得状态的效用值达到均衡，而且一定是贝尔曼方程组的唯一解，对应的策略也是最优的.

## 程序框架的构建

在了解价值迭代算法后，我们试图用$\text{Python}$程序模拟算法的迭代过程.
