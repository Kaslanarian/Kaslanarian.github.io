---
layout:     post
title:      用蒙特卡罗搜索树玩2048
subtitle:   过程记录
date:       2021-05-20
author:     Welt Xing
header-img: img/2048.jpg
catalog:    true
tags:
    - 编程技术
---

## 引言

我们尝试用蒙特卡洛树搜索方法设计和实现会玩2048游戏的智能程序。要求程序每1秒执行一个行动，并可视化智能程序玩2048游戏的过程。

> 2048游戏使用方向键让方块整体上下左右移动。如果两个带有相同数字的方块在移动中碰撞，则它们会合并为一个方块，且所带数字变为两者之和。每次移动时，会有一个值为2或者4的新方块出现，所出现的数字都是2的幂。当值为2048的方块出现时，游戏即胜利，该游戏因此得名。             ——[Wikipedia](https://en.wikipedia.org/wiki/2048_(video_game))

## 平台介绍

本实验程序于$\text{Ubuntu}20.04$上运行，实验代码所依赖的第三方$\text{Python}$库已经在`requirements.txt`文件中声明：

```python
# 获取代码后运行 pip install -r requirements.txt 以安装第三方库
gym
numpy
PyQt5
```

## 解读框架

我们拿到的文件（`game2048.py`）是一个随机决策的2048游戏框架：

```python
class Game2048Env(gym.Env):
    '''模拟2048游戏的内部逻辑，包括开始，移动，结束'''
    def __init__(self, render: bool = True):
        pass
	# ....
    
class Game2048GUI(QMainWindow):
    '''模拟游戏的外部逻辑，属于GUI内容，不需要我们修改'''
    def __init__(self, port, remote_port):
		pass
    # .....
    
if __name__ == '__main__':
    env = Game2048Env(True)
    env.seed(0)
    # render is automatically set to False for copied envs
    test_env = copy.deepcopy(env)
    test_env.setRender(True)
    # remember to call reset() before calling step()
    obs = env.reset()
    done = False
    while not done:
        # 每过0.2秒便在上下左右四个方向中随机选择一个作为action
        time.sleep(0.2)
        action = random.randint(0, 3)
        obs, rew, done, info = env.step(action)
        print(rew, done, info)
    env.close()
    test_env.setRender(False)
    test_env.close()
```

我们也可以删掉等待时间，手动输入`action`，自己去玩游戏；但我们的实验任务是让$\text{Agent}$根据现在的状态，选择出最优的`action`：

```python
action = Agent(env)
```

这里我们采用的是$\mathbb{UCT}$，也就是基于蒙特卡洛树搜索$(\text{MCTS})$和上置信界$\text{UCB}$的上置信区间搜索.

## $\mathbb{UCT}$算法相关

我们先来看看上置信区间搜索的前身：蒙特卡洛树搜索。它是$\text{Rémi Coulom}$在 2006 年于它的围棋人机对战引擎 「Crazy Stone」中首次发明并使用的的 ，并且取得了很好的效果。

这些树搜索算法其实和人类的思考方式有相似之处：当人在对弈或是玩2048这种游戏时，会根据感觉在大脑里大致筛选处几种**最可能**的走法，这其实就是MCTS算法的设计思路。

算法在有限的时间里（对弈时每一步都会有限制时间），不断进行下面的循环：

$$
\text{选择}\to\text{扩展}\to\text{仿真}\to\text{反向更新}
$$

> 在搜索树算法中，每一个节点都是游戏的一个状态，节点$a$与节点$b$之间的边表示从状态$a$到状态$b$的一个行动$action$.

![mct](https://i.stack.imgur.com/EieiQ.png)

### 选择

算法从根节点（对弈/游戏的某一状态）开始，在搜索树$T$（初始为$\emptyset$）向下搜索，直到到达一个节点$s$满足$s\notin T$；在上述过程中，我们需要确定一个选择分支的准则，在$\mathbb{UCT}$中，我们采用的是上置信界作为判断依据：

$$
a\gets\mathop{\arg\max}\limits_{a\in\mathcal{A}(s)}\bigg(Q(s,a)+c\sqrt{\dfrac{\log N(s)}{N(s,a)}}\bigg)
$$

这里的$Q(s,a)$是在$s$状态选择$a$行动获得的收益，$N(s)$是我们在有限时间的树搜索里到达$s$节点的次数，类似的，$N(s,a)$就是选择$a$行动到达$s(a)$的次数：

$$
N(s)=\sum_{a\in\mathcal{A}(s)}N(s,a)
$$

一方面，我们希望在遍历搜索树时倾向于奖赏$Q(s,a)$更大的分支；另一方面，我们希望探索那些不常去的地方，常数$c$便是用来控制对$\text{exploitation}$和$\text{exploration}$，也就是“利用”和“探索”之间的权衡.

### 扩展

上面的选择过程遇到一个不属于搜索树的节点便停止，原因是我们无法获取这个这个节点的信息，也就是无法根据上置信界判断最优行动；缺什么补什么，我们将这个节点“扩展”，也就是遍历该节点状态可行的行动，作为其子节点。这些节点是没被遍历过的，所以对任意的$a$有$N(s,a)=0$，而奖赏也是未知的，姑且设置为$0$. 最后我们将父节点加入到树里，因为可以根据子节点信息得到最优行动。

### 仿真（评价）

如果搜索树只进行上面两个策略：

$$
\text{选择}\leftrightarrows\text{扩展}
$$

似乎也可以搜索到指定深度，但由于我们的$Q$都是设置为0，那么选择最优行动时的标准就变为了

$$
a\gets\mathop{\arg\max}\limits_{a\in\mathcal{A}(s)}\sqrt{\dfrac{\log N(s)}{N(s,a)}}
$$

完全由遍历次数决定，这显然是不合理的。所以，对于扩展出的子节点，也需要拥有$Q$值。这里，我们采用某个**默认策略**来选择行动，直到到达制定深度。默认策略也称为**滚轮策略**（$\text{rollout policy}$）。经典的默认策略是随机的（部分解释了该算法和“蒙特卡罗方法”的联系），它为专家提供了一种方式，使得搜索偏向有希望的区域。

$\textbf{function }\text{ROLLOUT}(s,d):$\\
$\quad\textbf{if }d=0$\\
$\qquad\textbf{return }0$\\
$\quad a\gets\text{random}(\mathcal{A}(s))$\\
$\quad(s',r)\gets s(a)$\\
$\quad\textbf{return }r+\gamma\text{ROLLOUT}(s',d-1)$

### 反向更新

我们从新扩展的节点进行仿真，访问了一次该节点，得到了一个$Q$值，显然会对前面“选择”途中遍历节点的$Q$值和$N$产生影响，因此我们从新节点开始，依次向上更新这两个值。

### 伪代码版本

$\textbf{function }\text{UCTSearch}(s_0):$\\
$\quad\text{create root node }v_0\text{ with state }s_0.$\\
$\quad\text{while not out of time}:$\\
$\qquad v_l\gets\text{TreePolicy}(v_0)$\\
$\qquad \Delta\gets\text{DefaultPolicy}(v_l)$\\
$\qquad \text{backup}(v,\Delta)$\\
$\textbf{returns }a(\text{BestChild}(v_0,0))$

其中的函数细节需要我们去实现。

## 代码实现

相比于从零实现，在别人代码基础上进行修改更为困难，我们定义这样的一个叶节点：

```python
class TreeNode:
    '''
    表示搜索树中的一个节点
    '''
    def __init__(self, env:Game2048Env, parent):
        '''
        Parameters
        ----------
        env : 存储状态
        parent : 父节点
        
        Variables
        ---------
        children : 存储子节点的字典
        N : 访问此处
        Q : 初始效用
        '''
        pass
    
   	def expandable(self):
        '''判断是否可以扩展'''
        pass
    
    def expand(self):
        '''扩展一个叶节点'''
        pass
```

$\text{TreePolicy}$基于以下规则：

$\textbf{while }v\text{ is not terminal }\textbf{do}$\\
$\quad\textbf{if }v\text{ is expandable}$\\
$\qquad\textbf{returns }\text{expansion of }v$\\
$\quad\textbf{else}$\\
$\qquad v\gets\text{BestChild}(v,c)$

其中对于一个可扩展的节点，我们选择一步步将其完全展开，如果不可扩展，则采用上面所说的选择方法：

$$
\text{BuildChild}(v,c):\textbf{return }\mathop{\arg\max}\limits_{a\in\mathcal{A}(s)}\bigg(Q(s,a)+c\sqrt{\dfrac{\log N(s)}{N(s,a)}}\bigg)
$$

$\text{DefaultPolicy}$就是简单的$\text{ROLLOUT}$。反向更新中，我们将访问次数和效用进行更新：

$\textbf{while }v\text{ is not root}:$\\
$\quad N(v)\gets N(v)+1$\\
$\quad Q(v)\gets Q(v)+1$\\
$\quad v\gets v.parent$

按照要求，每次决策小号的时间是1s，故我们的搜索算法：

```python
from time import time

def MCTS(env: Game2048Env, d: int = 10, c: int = 100):
    root = TreeNode(env, None)
    t = time()
    while True:
        v = treePolicy(root)
        q = defaultPolicy(v, d)
        backup(v, q)
        if time() - t >= 1:
            break
    return bestAction(root, 0)
```

## 效果测试

```python
if __name__ == '__main__':
    d, c = 10, 100
    env = Game2048Env(True)
    env.seed(0)
    test_env = copy.deepcopy(env)
    test_env.setRender(True)
    obs = env.reset()
    done = False
    while not done:
        env_copy = deepcopy(env)
        action = MCTS(env_copy, c, d)
        obs, rew, done, info = env.step(action)
        print(rew, done, info)
    env.close()
    test_env.setRender(False)
    test_env.close()
```

运行上面的代码，我们进行运行测试：

<img src="/img/2048result.png" alt="img" style="zoom:50%;" />

可以看到作为一个2048玩家，$\mathbb{UCT}$已经有很高的水平，但仍然有发展空间.

## 算法改进

经高人指导，我们修改了$\text{ROLLOUT}$策略：

![rollout](/img/Agent/rollout.png)

也就是`defaultPolicy`的修改：

```python
def defaultPolicy(env):
    reward = env.score for i in range(d): 
        _, _, done, _ = env.step(randint(0, 3)) 
        if done: 
            break reward = env.score
    return reward
```

让我们再次测试⼀下程序，结果如下：

![success](/img/Agent/2048success.png)

说明策略的修正是正确的，也说明算法的强大。