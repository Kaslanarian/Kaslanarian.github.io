---
layout:     post
title:      SVM的Shrink技巧
subtitle:   Making large-scale SVM learning practical
date:       2021-07-12
author:     Welt Xing
header-img: img/genshin.jpg
catalog:    true
tags:
    - 机器学习
---

## <center>引言

 在解决支持向量机问题时，我们通常去解决其对偶问题，更具体地说，是去解一个长度为样本个数的$\alpha$向量。但在大样本学习过程中，这显然会导致数据存储和运算量过大的问题。幸运的是，SVM的解存在稀疏性，也就是最终模型仅与支持向量有关。在Joachims的《Making large-scale SVM learning practical》中，他提出的Shrinking方法能够有效缩短SVM的训练时间，并将其应用到他开发的SVM-light中，该方法同时也被libSVM所采用。

## <center>问题引入

从SVM的对偶问题OP1的目标函数

$$
W(\alpha)=\dfrac{1}2\alpha^\top Q\alpha-e^\top\alpha
$$

可以看出解决该问题至少要为矩阵$Q$和$\alpha$提供存储空间，而$Q_{ij}\equiv y_iy_jK(x_i,x_j)$，假设样本数为1000，那么我们至少需要为其提供$1000\times1000\times4+1000\times4$个字节，也就是约4 MB的存储空间，显然是非常不合理的。

Joachims从算法上基于以下事实提出了Shrinking方法来解决这一问题：

- 支持向量（SVs）的数量要比训练样本少得多；
- 许多支持向量对应的$\alpha_i$的值都是其上界$C$。

> 在硬间隔SVM中，所有$\alpha_i\gt0$对应的样本点$x_i$都是支持向量，它们都位于分类间隔上，反之也成立；而在软间隔SVM中，支持向量不一定全部分布在分类间隔上，在分类间隔中的的支持向量也被称作支持向量，这些向量的特征就是其对应的$\alpha_i=C$。

我们将样本向量分为三类：

1. $X$类：支持向量，但$0\lt\alpha_i\lt C$；
2. $Y$类：支持向量，但$\alpha_i=C$；
3. $Z$类：非支持向量，也就是$\alpha_i=0$.

因此我们将数据重排：

$$
\alpha=\begin{bmatrix}
\alpha_X\\\alpha_Y\\\alpha_Z
\end{bmatrix}=
\begin{bmatrix}
\alpha_X\\C\pmb 1\\\pmb 0
\end{bmatrix},y=\begin{bmatrix}
y_X\\
y_Y\\
y_Z
\end{bmatrix},Q=\begin{bmatrix}
Q_{XX}&Q_{XY}&Q_{XZ}\\
Q_{YX}&Q_{YY}&Q_{YZ}\\
Q_{ZX}&Q_{ZY}&Q_{ZZ}
\end{bmatrix}
$$

因此我们可以重写$W(\alpha)$：

$$
\begin{aligned}
W(\alpha)&=\dfrac{1}{2}\alpha^\top Q\alpha-C\pmb1^\top\alpha\\
&=\dfrac12\sum_{m\in\{X,Y,Z\}}\sum_{n\in\{X,Y,Z\}}\alpha_m^\top Q_{mn}\alpha_n-C\pmb1^\top\alpha_X-C\pmb1^\top\alpha_Y-C\pmb1^\top\alpha_Z\\
&=\dfrac12\sum_{m\in\{X,Y\}}\sum_{n\in\{X,Y\}}\alpha_m^\top Q_{mn}\alpha_n-C\pmb1^\top\alpha_X-C\pmb1^\top\alpha_Y\\
&=\dfrac{1}{2}\alpha_X^\top Q_{XX}\alpha_{X}+C\alpha^\top_XQ_{XY}\pmb1+\dfrac{1}{2}C^2\pmb1^\top Q_{YY}\pmb1-C\alpha_X^\top\pmb1-\vert Y\vert C\\
&=\dfrac{1}{2}\alpha_X^\top Q_{XX}\alpha_{X}+C\alpha^\top_X\big(Q_{XY}\pmb 1-\pmb1 \big)+\dfrac{1}{2}C^2\pmb1^\top Q_{YY}\pmb1-\vert Y\vert C
\end{aligned}
$$

考虑到后面两项为常数，我们重写对偶问题<span id="OP2">OP2</span>：

$$
\begin{aligned}
\min_{\alpha_X}\quad&\dfrac{1}{2}\alpha_X^\top Q_{XX}\alpha_{X}+C\alpha^\top_X\big(Q_{XY}\pmb 1-\pmb1 \big)\\
\text{subject to}\quad&\alpha_X^\top y_X+C\pmb1^\top y_Y=0\\
&0\le\alpha_X\le C\pmb 1
\end{aligned}
$$

可以发现，问题的规模大幅度减小，矩阵和向量的维数数量级只由支持向量个数决定。这一过程便称为收缩（Shrinking）。

## <center>Shrinking的启发式算法

虽然我们可以通过Shrinking来缩小问题规模，但它基于我们已知$\alpha_i$是属于$\alpha_X$、$\alpha_Y$或$\alpha_Z$，这对于算法是很难判定的。到目前为止，还不清楚该算法如何识别哪些样本可以消除，也就是对应的$\alpha_i$为$0$或$C$的样本。我们希望在优化过程的早期找到一些条件，这些条件表明某些变量最终会达到一个界限。由于充分条件未知，采用基于拉格朗日乘子估计的启发式方法。

设$A$是当前满足$\alpha_i\in(0,C)$的集合：

$$
A=\{\alpha_i\vert0\lt\alpha\lt C,i=1,\cdots,l\}
$$

然后设估计值$\lambda^{eq}$：

$$
\lambda^{eq}=\dfrac{1}{\vert A\vert}\sum_{i\in A}\bigg[y_i-\sum_{j=1}^l\alpha_jy_jK(\pmb x_i,\pmb x_j) \bigg]
$$

注意到我们可以用$\lambda^{eq}$作为SVM决策函数中的bias，也就是$b$。以此设置拉格朗日乘子，也就是$\alpha_i$的上下界：

$$
\lambda_i^{lo}=y_i\bigg(\bigg[\sum_{j=1}^l\alpha_jy_jK(\pmb x_i,\pmb x_j)\bigg]+\lambda^{eq} \bigg)-1\\
\lambda_i^{up}=-y_i\bigg(\bigg[\sum_{j=1}^l\alpha_jy_jK(\pmb x_i,\pmb x_j)\bigg]+\lambda^{eq} \bigg)+1\\
$$

给定$h$一正整数，此时考虑SMO迭代过程的前$h$个循环，在这$h$个循环中，对于某个$i$，如果都有$\lambda_i^{lo}>0$且$\lambda_i^{up}>0$（也可以用一个极小阈值$\varepsilon$代替0），那么我们就有信心将其删除。也就是说$\alpha_i$已经是最优的，它们可以被固定，从而不需要对其梯度等值进行计算。

由于启发式算法没有定理去证明合理性，必然会存在删错了的情况。因此在[OP2](#OP2)收敛后，对被删除变量的最优条件进行检查；如有必要，则会在这一次迭代中重新进行一次优化。

