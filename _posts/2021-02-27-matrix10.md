---
layout:     post
title:      C++构建矩阵类(9)
subtitle:   求解特征向量从而实现特征分解
date:       2021-02-26
author:     Welt Xing
header-img: img/matrix_equations.png
catalog:    true
categories: matrix-c
tags:
    - matrix-c
    - 线性代数
---

# 特征值，特征向量和特征分解

## 特征向量的求解

前面的文章已经告诉我们，如果知道矩阵$A$的一个特征值$\lambda$，那么我们只需要解齐次线性方程组

$$
(A-\lambda E)x=0
$$

由于$\det(A-\lambda E)=0$，所以解必然是$n-r$个线性无关的向量：$\beta_1,\beta_2,...\beta_{n-r}$，它们的任意线性组合都是$\lambda$对应的特征向量：

$$
A\beta_1=\lambda\beta_1,A\beta_2=\lambda\beta_2\to
\begin{aligned}
A(k_1\beta_1+k_2\beta_2)
&=k_1A\beta_1+k_2A\beta_2\\
&=k_1\lambda\beta_1+k_2\lambda\beta_2\\
&=\lambda(k_1\beta_1+k_2\beta_2)\\
\end{aligned}
$$

我们可以据此用程序模拟计算出矩阵$A$的特征向量：

```cpp
matrix_pair eigen_vector(Matrix m) {
    int n = m.get_column_number();

    bool is_first = true;

    // 删去特征值vector中重复的部分，也就是集合化
    vec2double unique_eigens = to_set(eigens);

    // 特征值按绝对值大小排序，符合MATLAB的规范
    std::sort(unique_eigens.begin(), unique_eigens.end(),
              [](double a, double b) { return std::abs(a) > std::abs(b); });

    Matrix ret, solution;
    for (double eigen_value : unique_eigens) {
        // 对应解方程组 : (A-λI)x = 0
        solution = linear_solve(m - eigen_value * eye(n));
        for (int i = 0; i < solution.get_column_number(); i++) {
            double norm_col = 0;
            for (int j = 0; j < solution.get_row_number(); j++) {
                norm_col += solution[j][i] * solution[j][i];
            }
            norm_col = sqrt(norm_col);
            // 将每一列(每一个特征向量)归一化
            for (int j = 0; j < solution.get_row_number(); j++) {
                solution[j][i] /= norm_col;
            }
        }
        if (is_first) {
            ret = solution;
            is_first = false;
        } else {
            ret = *cat(&ret, &solution);
        }
    }
    // 返回一个矩阵pair：特征值构成的对角阵和对应的特征向量
    return matrix_pair(diag(eigens), ret);
}
```

我们运行下面的测试程序：

```cpp
using namespace std;
int main() {
    Matrix A = Matrix(3, 3);
    cin >> A;
    matrix_pair result = eigen_vector(A);
    cout << result.first << endl
         << result.second << endl;
    return 0;
}
```

计算结果：![结果](/img/eigen_vec_test.png)

我们用`OCATVE`去计算该矩阵的特征值和特征向量以验证结果：

```matlab
>> A = [6 2 4; 2 3 2; 4 2 6];
>> [X, Y] = eig(A)
X = 
    0.59628     0.44721     0.66667
    0.29814    -0.89443     0.33333
   -0.74536     0.00000     0.66667
Y = 
    2.0000      0           0
    0           2.00000     0
    0           0           11.0000
```

发现$\lambda=2$的一个特征向量不一样：

$$
\begin{bmatrix}
-0.7071\\0.0000\\0.7071
\end{bmatrix}\to
\begin{bmatrix}
-\frac{\sqrt{2}}{2}\\0\\\frac{\sqrt{2}}{2}
\end{bmatrix}
$$

只需要验证：

$$
\begin{bmatrix}
6-2&2&4\\
2&3-2&2\\
4&2&6-2\\
\end{bmatrix}\begin{bmatrix}
-\frac{\sqrt{2}}{2}\\0\\\frac{\sqrt{2}}{2}
\end{bmatrix}=0
$$

即可，事实上也的确是0，说明`matrix-c`的计算结果是正确的

## 特征分解

对于**可对角化矩阵**$A$，矩阵可以被分解为特征向量和特征值的积：

$$
\begin{aligned}
A
&=Q\Lambda Q^{-1}\\
&=\begin{bmatrix}
\alpha_1&\alpha_2&\cdots&\alpha_n
\end{bmatrix}\begin{bmatrix}
\lambda_1\\
&\lambda_2\\
&&\ddots\\
&&&\lambda_n\\
\end{bmatrix}\begin{bmatrix}
\alpha_1\\
\alpha_2\\
\vdots\\
\alpha_n\\
\end{bmatrix}^{-1}
\end{aligned}
$$

$\alpha_i$就是第$i$个特征值$\lambda_i$对应的特征向量，习惯上$ \|\alpha_i\|=1 $

只要一个$n\times n$矩阵有$n$个特征向量，那么它就是可对角化的，所以当我们已经有`eigen_vector`函数以求得特征值和特征向量后，对矩阵进行特征分解就比较容易了：

```cpp
matrix_tuple EIGENdecomposition(Matrix A) {
    matrix_pair QLambda = eigen_vector(A);
    matrix_tuple ret;
    WarnInterrupt(A.get_column_number() == QLambda.first.get_column_number(),
                  "矩阵无法在实数域上对角化",
                  ret);
    std::get<0>(ret) = QLambda.second;
    std::get<1>(ret) = QLambda.first;
    std::get<2>(ret) = inv(QLambda.second);
    return ret;
}
```

仍使用矩阵

$$
\begin{bmatrix}
6&2&4\\2&3&2\\4&2&6
\end{bmatrix}
$$

作为测试样例：

```cpp
int main() {
    Matrix A = Matrix(3, 3);
    cin >> A;
    matrix_tuple result = EIGENdecomposition(A);
    Matrix X = get<0>(result);
    Matrix Y = get<1>(result);
    Matrix Z = get<2>(result);
    cout << A << " = " << X << " * " << Y << " * " << Z << endl;
    // 对结果进行验证：
    cout << A - X * Y * Z << endl; // 理论上会是一个全零矩阵
    return 0;
}
```

运行结果：![测试](/img/eigen_decom_test.png)

也就是：

$$
\begin{bmatrix}
6&2&4\\2&3&2\\4&2&6
\end{bmatrix}=
\begin{bmatrix}
\frac{2}{3}&-\frac{\sqrt{5}}{5}&-\frac{\sqrt{2}}{2}\\
\frac{1}{3}&\frac{2\sqrt{5}}{5}&0\\
\frac{2}{3}&0&\frac{\sqrt{2}}{2}
\end{bmatrix}\begin{bmatrix}
11&&\\
&2&\\
&&2
\end{bmatrix}\begin{bmatrix}
\frac{2}{3}&-\frac{\sqrt{5}}{5}&-\frac{\sqrt{2}}{2}\\
\frac{1}{3}&\frac{2\sqrt{5}}{5}&0\\
\frac{2}{3}&0&\frac{\sqrt{2}}{2}
\end{bmatrix}^{-1}
$$

但如果输入一个不可对角化矩阵，我们需要抛出异常：

```cpp
// 假如特征向量数量小于n，就会抛出异常
WarnInterrupt(A.get_column_number() == QLambda.first.get_column_number(),
         "矩阵无法在实数域上对角化", ret);
```

## 对称矩阵与特征向量

我们依旧讨论这个例子，$\lambda=11$对应的特征向量是$\alpha_1=\begin{bmatrix}\frac{2}{3}&\frac{1}{3}&\frac{2}{3}\end{bmatrix}^\top$，$\lambda=2$对应的特征向量是$\alpha_2=\begin{bmatrix}-\frac{\sqrt{5}}{5}&\frac{2\sqrt{5}}{5}&0\end{bmatrix}^\top$和$\alpha_3=\begin{bmatrix}-\frac{\sqrt{2}}{2}&0&\frac{\sqrt{2}}{2}\end{bmatrix}^\top$，我们可以发现：$\alpha_2,\alpha_3$都和$\alpha_1$正交，再看看非对称矩阵

$$
\begin{bmatrix}
2&5&1\\
7&2&3\\
1&0&2\\
\end{bmatrix}
$$

其被分解成：

$$
\begin{bmatrix}
2&5&1\\
7&2&3\\
1&0&2\\
\end{bmatrix}=\begin{bmatrix}
0.63451&-0.66121&-0.38182\\
0.76611&0.74143 &-0.15041\\
0.10237&0.11440 &0.91191\\
\end{bmatrix}\begin{bmatrix}
8.19839&0&0\\0&1.58129&0\\0&0&-3.77968
\end{bmatrix}\begin{bmatrix}
0.76395&  0.61625&  0.42152\\
-0.78676&  0.68062& -0.21716\\
0.01294& -0.15456&  1.07652\\
\end{bmatrix}
$$

可以发现不同特征向量之间不是正交的。我们尝试证明“对称阵不同特征值对应的特征向量两两正交”。

首先我们有：

$$
Ax_i=\lambda_ix_i\tag{1}
$$

$$
Ax_j=\lambda_jx_j\tag{2}
$$

其中$\lambda_i\neq\lambda_j$，将(1)的两边左乘以$x_j^\top$：

$$
x_j^\top Ax_i=\lambda_ix_j^\top x_i\tag{3}
$$

因为矩阵$A$是一个对称矩阵，可以对式(3)的左边做如下变换：

$$
x_j^\top Ax_i=x_j^\top A^\top x_i=(Ax_j)^\top x_i\tag{4}
$$

将式(2)带入(4):

$$
x_j^\top Ax_i=(Ax_j)^\top x_i=\lambda_jx_j^\top x_i\tag{5}
$$

结合(3)，我们推出：

$$
\lambda_i x_j\top x_i = \lambda_j x_j^\top x_i\to(\lambda_i -
\lambda_j) x_j^\top x_i = 0\tag{6}
$$

因为$\lambda_i\neq\lambda_j$，所以必有：

$$
x_j^\top x_i=0\tag{7}
$$

所以定理得到证明。

## 与OCTAVE发生差异的原因

在$\text{OCTAVE}$的结果里，我们会发现，一个特征值对应的多个特征向量，也就是$\alpha_2$和$\alpha_3$被正交化了，所以使得$\begin{bmatrix}\alpha_1&\alpha_2&\alpha_3\end{bmatrix}$成为了一个正交矩阵：

$$
\begin{bmatrix}
\alpha_1&\alpha_2&\alpha_3
\end{bmatrix}\begin{bmatrix}
\alpha_1^\top\\\alpha_2^\top\\\alpha_3^\top
\end{bmatrix}=I
$$

所以使得：

$$
A=Q\Lambda Q^{-1}=Q\Lambda Q^\top
$$

$A$和$\Lambda$既是相似的，又是合同的，正交化将两种关系性质统一起来。

受此启发，我们也可以对代码进行修改，原来的解特征方程是这样的：

```cpp
solution = linear_solve(m - eigen_value * eye(n));
```

我们可以改成：

```cpp
solution = SchmidtFullRank(linear_solve(m - eigen_value * eye(n))).second;
```

`SchmidtFullRank`是对列向量进行施密特正交化的函数。

我们再看看修改后的求解结果：

![正交化处理后的分解](/img/eigen_decom_test3.png)

## 总结

我们现在已经实现了特征分解，接下来我们就可以进入矩阵分解的部分，包括之前提到的$QR$分解，以及著名的$SVD$分解。

- 上一篇：[matrix-c求解矩阵的实特征值](https://welts.xyz/matrix-c/2021/02/26/matrix9/)

- 下一篇：[matrix-c实现矩阵的LU分解](https://welts.xyz/matrix-c/2021/02/28/matrix11/)
